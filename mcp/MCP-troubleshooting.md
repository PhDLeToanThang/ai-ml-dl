**[Ph·∫ßn 7: Troubleshooting Debug Testing Check list:](#ph·∫ßn-7)**

---

## Ph·∫ßn 7

### Troubleshooting Debug Testing Check list

#### Vi·ªác c·∫£i ti·∫øn b·ªï sung Model AI c√≥ c√°c kh·∫£ nƒÉng m·ªõi- quan tr·ªçng nh∆∞ Vision Lang

>>> FAQ 1. Khi download Qwen3-VL-8B-Instruct cho LM Studio 0.3.31 th√†nh c√¥ng nh∆∞ng khi load model n√†y th√¨ b·ªã b√°o l·ªói:
error loading model: error loading model architecture: unknown model architecture: 'Qwen3-VL-8B-Instruct h√£y h∆∞·ªõng d·∫´n c√°ch n√†o ƒë·ªÉ t√¥i c√≥ th·ªÉ tr√°nh l·ªói ki·ªÉu n√†y ? v√≠ d·ª•: b·∫≠t th√¥ng tin v·ªÅ ARCHITECTURE tr∆∞·ªõc trong g√≥i download n·∫øu tr·ªëng t√¥i s·∫Ω b·ªè qua.

C·∫£m ∆°n b·∫°n ƒë√£ chia s·∫ª v·∫•n ƒë·ªÅ. L·ªói *"unknown model architecture"* th∆∞·ªùng x·∫£y ra khi LM Studio kh√¥ng th·ªÉ nh·∫≠n di·ªán ƒë·ªãnh d·∫°ng ho·∫∑c c·∫•u tr√∫c c·ªßa model do b·∫°n t·∫£i v·ªÅ. D∆∞·ªõi ƒë√¢y l√† h∆∞·ªõng d·∫´n chi ti·∫øt ƒë·ªÉ gi√∫p b·∫°n tr√°nh l·ªói n√†y:

---

### ‚úÖ **1. Ki·ªÉm tra th√¥ng tin architecture tr∆∞·ªõc khi t·∫£i model**
Tr∆∞·ªõc khi download, h√£y ƒë·∫£m b·∫£o:
- Model c√≥ file `config.json` (ho·∫∑c t∆∞∆°ng t·ª±) trong th∆∞ m·ª•c g·ªëc.
- File `config.json` ch·ª©a c√°c th√¥ng tin nh∆∞:
  ```json
  {
    "architectures": ["LlamaForCausalLM"],
    "model_type": "llama",
    "hidden_size": 4096,
    ...
  }
  ```
- N·∫øu b·∫°n ƒëang t·∫£i model t·ª´ ngu·ªìn b√™n ngo√†i (kh√¥ng ph·∫£i LM Studio market), h√£y ki·ªÉm tra xem n√≥ c√≥ t∆∞∆°ng th√≠ch v·ªõi LM Studio phi√™n b·∫£n b·∫°n ƒëang d√πng kh√¥ng.

---

### ‚úÖ **2. C√°ch kh·∫Øc ph·ª•c l·ªói hi·ªán t·∫°i**
V√¨ b·∫°n ƒë√£ t·∫£i v·ªÅ `Qwen3-VL-8B-Instruct` nh∆∞ng LM Studio kh√¥ng nh·∫≠n di·ªán architecture, b·∫°n c√≥ th·ªÉ:

#### üîß T·ª± ki·ªÉm tra c·∫•u tr√∫c th∆∞ m·ª•c
ƒê·∫£m b·∫£o th∆∞ m·ª•c model c·ªßa b·∫°n tr√¥ng nh∆∞ th·∫ø n√†y:
```
qwen3-vl-8b-instruct/
‚îú‚îÄ‚îÄ config.json          # B·∫Øt bu·ªôc, ph·∫£i ch·ª©a "architectures"
‚îú‚îÄ‚îÄ model.pth            # ho·∫∑c file weights (t√πy ƒë·ªãnh d·∫°ng)
‚îú‚îÄ‚îÄ tokenizer.model      # B·∫Øt bu·ªôc n·∫øu kh√¥ng d√πng tokenizer ri√™ng
‚îî‚îÄ‚îÄ ...                  # C√°c file kh√°c li√™n quan
```

N·∫øu thi·∫øu `config.json` ‚Üí LM Studio s·∫Ω kh√¥ng bi·∫øt c√°ch load.

---

### ‚úÖ **3. C√°ch tr√°nh l·ªói t∆∞∆°ng lai (nh∆∞ b·∫°n ƒë·ªÅ xu·∫•t)**
ƒê·ªÉ t·ª± ƒë·ªông h√≥a ki·ªÉm tra, b·∫°n c√≥ th·ªÉ:
1. T·∫°o script ki·ªÉm tra c·∫•u tr√∫c model tr∆∞·ªõc khi import
2. B·ªè qua model n·∫øu thi·∫øu th√¥ng tin architecture

V√≠ d·ª• script ƒë∆°n gi·∫£n (Python):
```python
import os
import json

def validate_model_structure(model_dir):
    required_files = {"config.json", "tokenizer.model"}
    files = set(os.listdir(model_dir))
    
    if not required_files.issubset(files):
        missing = required_files - files
        raise ValueError(f"Thi·∫øu file b·∫Øt bu·ªôc: {missing}")
    
    # Ki·ªÉm tra config.json
    config_path = os.path.join(model_dir, "config.json")
    with open(config_path, 'r') as f:
        config = json.load(f)
        
        if "architectures" not in config or len(config["architectures"]) == 0:
            raise ValueError("Kh√¥ng t√¨m th·∫•y architecture trong config.json")
    
    print("‚úÖ Model h·ª£p l·ªá!")

# S·ª≠ d·ª•ng
validate_model_structure("/path/to/qwen3-vl-8b-instruct")
```

---

### ‚úÖ **4. G·ª£i √Ω cho ng∆∞·ªùi d√πng t·∫£i model**
N·∫øu b·∫°n ƒëang chia s·∫ª model v·ªõi ng∆∞·ªùi kh√°c, h√£y bao g·ªìm:
- File `config.json` ƒë·∫ßy ƒë·ªß
- H∆∞·ªõng d·∫´n ng·∫Øn g·ªçn: "ƒê·∫£m b·∫£o c√≥ file config.json v√† tokenizer.model tr∆∞·ªõc khi import"
- Ki·ªÉm tra t∆∞∆°ng th√≠ch v·ªõi LM Studio version (0.3.31 ch·ªâ h·ªó tr·ª£ m·ªôt s·ªë format)

---

### ‚úÖ **K·∫øt lu·∫≠n**
L·ªói c·ªßa b·∫°n do LM Studio kh√¥ng nh·∫≠n di·ªán ƒë∆∞·ª£c architecture, th∆∞·ªùng do:
- Thi·∫øu `config.json`
- Architecture name kh√¥ng ƒë√∫ng v·ªõi LM Studio h·ªó tr·ª£
- File b·ªã h·ªèng ho·∫∑c thi·∫øu component

üëâ **Gi·∫£i ph√°p d√†i h·∫°n:** Lu√¥n ki·ªÉm tra c·∫•u tr√∫c model tr∆∞·ªõc khi import, ƒë·∫∑c bi·ªát l√† th√¥ng tin architecture v√† tokenizer.

---

B·∫°n mu·ªën m√¨nh gi√∫p b·∫°n t·∫°o m·ªôt script t·ª± ƒë·ªông ch·ªçn l·ªçc model h·ª£p l·ªá cho LM Studio 0.3.31 kh√¥ng?

>>> FAQ 2. T√¥i c√≥ th·∫•y file config.json v√† c√≥ t√™n Architecture , v√≠ d·ª•:
{
"architectures": [
"Qwen3VLMoeForConditionalGeneration"
],
"dtype": "bfloat16",
"image_token_id": 151655,
"model_type": "qwen3_vl_moe",
"text_config": {
"attention_bias": false,
"attention_dropout": 0.0,
"bos_token_id": 151643,
nh∆∞ng LM Studio c√≥ v·∫ª ƒë√£ kh√¥ng ch·∫•p nh·∫≠n t√™n ki·∫øn tr√∫c Qwen3VLMoe m√† ch·ªâ c√≥ LLM, qwen, phi2, LLAMA, glm4, q∆∞en3 ‚Ä¶ v·∫≠y theo b·∫°n c√≥ c·∫•ch gi·∫£i ph√°p n√†o ƒë·ªÉ LM Studio ch·∫•p nh·∫≠n ki·∫øn tr√∫c m·ªõi n√†y ch·∫°y tr√™n LM Studio desktop chat ho·∫∑c b√™n Open-Web UI c√≥ th·ªÉ ch·∫°y direct Model AI ƒë√≥.

---

C·∫£m ∆°n b·∫°n ƒë√£ chia s·∫ª chi ti·∫øt! T·ª´ th√¥ng tin b·∫°n cho, t√¥i hi·ªÉu r√µ v·∫•n ƒë·ªÅ:

## üß† T√≥m t·∫Øt v·∫•n ƒë·ªÅ
LM Studio kh√¥ng nh·∫≠n di·ªán ƒë∆∞·ª£c architecture `"Qwen3VLMoeForConditionalGeneration"` v√¨:
- ƒê√¢y l√† m·ªôt architecture t·ª± ƒë·ªãnh nghƒ©a (kh√¥ng ph·∫£i trong danh s√°ch h·ªó tr·ª£ c·ªßa LM Studio)
- LM Studio 0.3.31 ch·ªâ h·ªó tr·ª£ m·ªôt s·ªë architecture ph·ªï bi·∫øn (LLaMA, Qwen, Phi, GLM4...)

## ‚úÖ Gi·∫£i ph√°p kh·∫Øc ph·ª•c

### 1. **T·ª± c·∫•u h√¨nh architecture cho LM Studio**
B·∫°n c√≥ th·ªÉ t·∫°o file `model_config.json` trong th∆∞ m·ª•c model v·ªõi n·ªôi dung nh∆∞ sau:

```json
{
  "name": "Qwen3-VL-8B-Instruct",
  "type": "custom",  // ho·∫∑c "qwen3_vl_moe"
  "architecture": "Qwen3VLMoeForConditionalGeneration",
  "config_files": [
    "config.json",
    "tokenizer.model"
  ],
  "library_name": "transformers",  // ho·∫∑c t√™n th∆∞ vi·ªán b·∫°n ƒëang d√πng
  "library_version": "4.35.0"      // phi√™n b·∫£n library t∆∞∆°ng ·ª©ng
}
```

Sau ƒë√≥ ƒë·∫∑t file n√†y v√†o c√πng c·∫•p v·ªõi `config.json` v√† `tokenizer.model`.

---

### 2. **T·ª± m·ªü r·ªông h·ªó tr·ª£ architecture trong LM Studio**
N·∫øu b·∫°n mu·ªën LM Studio ch√≠nh th·ª©c h·ªó tr·ª£ `Qwen3VLMoeForConditionalGeneration`, c√≥ th·ªÉ:
1. Tham gia c·ªông ƒë·ªìng LM Studio (GitHub/GitLab)
2. G·ª≠i PR th√™m v√†o file:  
   `lm-studio/server/app/models/model_architectures.py`
3. C·∫≠p nh·∫≠t danh s√°ch architecture h·ªó tr·ª£

---

### 3. **Ch·∫°y tr·ª±c ti·∫øp v·ªõi Open-Web UI**
N·∫øu b·∫°n mu·ªën ch·∫°y model m√† kh√¥ng qua LM Studio, h√£y s·ª≠ d·ª•ng **Open-WebUI** (h·ªó tr·ª£ nhi·ªÅu architecture h∆°n):

1. T·∫£i Open-WebUI t·ª´: https://github.com/open-webui/open-webui
2. ƒê∆∞a model v√†o folder `models/`
3. C·∫•u h√¨nh trong file:  
   `openwebui/config.py` ho·∫∑c `config.json`

V√≠ d·ª• c·∫•u h√¨nh:
```json
{
  "models": [
    {
      "name": "Qwen3-VL-8B-Instruct",
      "type": "transformers",
      "model_path": "/path/to/qwen3-vl-8b-instruct",
      "architecture": "Qwen3VLMoeForConditionalGeneration"
    }
  ]
}
```

---

### 4. **Ki·ªÉm tra t∆∞∆°ng th√≠ch v·ªõi LM Studio 0.3.31**
N·∫øu b·∫°n mu·ªën ch·∫°y tr√™n LM Studio desktop:
- ƒê·∫£m b·∫£o phi√™n b·∫£n model t∆∞∆°ng th√≠ch v·ªõi transformers v4.35+
- Ki·ªÉm tra xem file `config.json` c√≥ ƒë·ªãnh d·∫°ng ƒë√∫ng kh√¥ng

---

## üöÄ G·ª£i √Ω n√¢ng c·∫•p
N·∫øu b·∫°n th∆∞·ªùng xuy√™n d√πng c√°c model m·ªõi, t√¥i khuy√™n b·∫°n n√™n:
1. S·ª≠ d·ª•ng **Open-WebUI** thay v√¨ LM Studio (h·ªó tr·ª£ nhi·ªÅu architecture h∆°n)
2. T·∫°o script ki·ªÉm tra t·ª± ƒë·ªông tr∆∞·ªõc khi import model v√†o LM Studio
3. ƒê√≥ng g√≥p architecture l√™n LM Studio community ƒë·ªÉ h·ªó tr·ª£ cho c·ªông ƒë·ªìng

---

#### Gi·∫£i ph√°p x√°c ƒë·ªãnh v√† c√°ch s·ª≠a do AI g·ª£i √Ω:

>>> FAQ 3. B·∫°n t·∫°o m·ªôt script t·ª± ƒë·ªông ch·ªçn l·ªçc model h·ª£p l·ªá cho LM Studio 0.3.31 v√† h∆∞·ªõng d·∫´n thao t√°c tr√™n LM Studio d·ªÖ d√πng ? v√† t·∫°o m·ªôt script ki·ªÉm tra t·ª± ƒë·ªông cho model c√≥ t√™n do b·∫°n nh·∫≠p v√†o sau ƒë√≥ c√≥ th·ªÉ ch·∫°y v√† chat tr√™n open webui ƒë·ªÉ x√°c ƒë·ªãnh

---

T√¥i s·∫Ω gi√∫p b·∫°n t·∫°o **hai script**:
1. Script ki·ªÉm tra model h·ª£p l·ªá cho LM Studio 0.3.31
2. Script chuy·ªÉn model sang Open-WebUI ƒë·ªÉ chat tr·ª±c ti·∫øp

---

## üìÅ 1. Script Ki·ªÉm Tra Model H·ª£p L·ªá cho LM Studio

```python
import os
import json
from typing import Optional, Dict

def validate_lm_studio_model(model_dir: str, lmstudio_version: str = "0.3.31") -> Dict[str, bool]:
    """
    Ki·ªÉm tra model c√≥ th·ªÉ ch·∫°y tr√™n LM Studio kh√¥ng
    Tr·∫£ v·ªÅ dict tr·∫°ng th√°i ki·ªÉm tra
    """
    result = {
        "valid": False,
        "details": {}
    }
    
    # 1. Ki·ªÉm tra t·ªìn t·∫°i file c·∫ßn thi·∫øt
    required_files = {"config.json", "tokenizer.model"}
    files_present = set(os.listdir(model_dir))
    
    missing_files = list(required_files - files_present)
    result["details"]["missing_files"] = missing_files
    
    if missing_files:
        result["valid"] = False
        return result
    
    # 2. Ki·ªÉm tra config.json
    config_path = os.path.join(model_dir, "config.json")
    
    try:
        with open(config_path, 'r') as f:
            config = json.load(f)
        
        # Ki·ªÉm tra architecture h·ªó tr·ª£ cho LM Studio 0.3.31
        supported_architectures = {
            "LlamaForCausalLM", "QwenForCausalLM", "PhiForCausalLM",
            "GLM4ForCausalLM", "AutoModelForCausalLM"
        }
        
        if not config.get("architectures") or len(config["architectures"]) == 0:
            result["details"]["config_error"] = "Kh√¥ng t√¨m th·∫•y architecture trong config.json"
            return result
        
        # Ki·ªÉm tra xem architecture c√≥ ƒë∆∞·ª£c LM Studio h·ªó tr·ª£ kh√¥ng
        architectures_present = set(config["architectures"])
        
        if not architectures_present.intersection(supported_architectures):
            result["valid"] = False
            result["details"]["architecture_not_supported"] = (
                f"Architecture '{config['architectures'][0]}' kh√¥ng ƒë∆∞·ª£c LM Studio {lmstudio_version} h·ªó tr·ª£"
            )
            return result
        
        result["valid"] = True
        result["details"]["architecture"] = config["architectures"][0]
        
    except Exception as e:
        result["valid"] = False
        result["details"]["config_error"] = str(e)
    
    return result


# V√≠ d·ª• s·ª≠ d·ª•ng
if __name__ == "__main__":
    model_paths = [
        "/path/to/qwen3-vl-8b-instruct",  # Thay b·∫±ng ƒë∆∞·ªùng d·∫´n th·ª±c t·∫ø
        "/path/to/phi-2",
        "/path/to/glm4"
    ]
    
    lmstudio_version = "0.3.31"
    
    for model_path in model_paths:
        print(f"Ki·ªÉm tra model: {model_path}")
        validation = validate_lm_studio_model(model_path, lmstudio_version)
        
        if validation["valid"]:
            print("‚úÖ Model h·ª£p l·ªá!")
        else:
            print("‚ùå Model kh√¥ng h·ª£p l·ªá:")
            for k, v in validation["details"].items():
                print(f" - {k}: {v}")
```

## üìã H∆∞·ªõng D·∫´n Thao T√°c tr√™n LM Studio

### B∆∞·ªõc 1: Copy model v√†o folder `models/`
```
LM Studio Installation Folder/
‚îî‚îÄ‚îÄ models/
    ‚îú‚îÄ‚îÄ qwen3-vl-8b-instruct/   ‚Üê ƒê·∫∑t file ·ªü ƒë√¢y
    ‚îî‚îÄ‚îÄ ... other models ...
```

### B∆∞·ªõc 2: Ch·ªçn model trong LM Studio
1. M·ªü LM Studio ‚Üí `Models` tab
2. T√¨m model b·∫°n v·ª´a copy
3. Nh·∫•n `Add to Library`
4. Ch·ªçn architecture ph√π h·ª£p (n·∫øu c√≥ nhi·ªÅu l·ª±a ch·ªçn)

### B∆∞·ªõc 3: Chat v·ªõi model
- Sau khi th√™m v√†o library
- M·ªü chat interface
- Ch·ªçn model t·ª´ dropdown menu
- Nh·∫≠p c√¢u h·ªèi v√† nh·∫•n Enter

---

## üìÅ 2. Script Chuy·ªÉn Model sang Open-WebUI ƒë·ªÉ Chat Tr·ª±c Ti·∫øp

```python
import os
import json
from datetime import datetime

def convert_to_open_webui_format(model_dir: str, model_name: str) -> None:
    """
    Chuy·ªÉn ƒë·ªïi model th√†nh ƒë·ªãnh d·∫°ng Open-WebUI y√™u c·∫ßu
    T·∫°o file config.json cho Open-WebUI
    """
    # Ki·ªÉm tra y√™u c·∫ßu
    required_files = {"config.json", "tokenizer.model"}
    files_present = set(os.listdir(model_dir))
    
    missing_files = list(required_files - files_present)
    
    if missing_files:
        raise FileNotFoundError(
            f"Thi·∫øu file b·∫Øt bu·ªôc: {missing_files}. "
            "Vui l√≤ng ki·ªÉm tra l·∫°i model"
        )
    
    # T·∫°o folder target n·∫øu ch∆∞a c√≥
    target_dir = os.path.join(model_dir, "openwebui")
    os.makedirs(target_dir, exist_ok=True)
    
    # ƒê·ªçc config.json g·ªëc
    config_path = os.path.join(model_dir, "config.json")
    with open(config_path, 'r') as f:
        original_config = json.load(f)
    
    # T·∫°o config m·ªõi cho Open-WebUI
    new_config = {
        "model_name": model_name,
        "type": "transformers",
        "model_path": model_dir,
        "architecture": original_config["architectures"][0],
        "library_name": "transformers",
        "library_version": "4.35.0",  # ƒê·∫£m b·∫£o phi√™n b·∫£n ph√π h·ª£p
        "added_at": datetime.now().isoformat()
    }
    
    # L∆∞u file m·ªõi
    new_config_path = os.path.join(target_dir, "config.json")
    with open(new_config_path, 'w') as f:
        json.dump(new_config, f, indent=2)
    
    print(f"‚úÖ ƒê√£ chuy·ªÉn ƒë·ªïi th√†nh c√¥ng!")
    print(f"T·ªáp config cho Open-WebUI: {new_config_path}")


# V√≠ d·ª• s·ª≠ d·ª•ng
if __name__ == "__main__":
    model_dir = "/path/to/qwen3-vl-8b-instruct"  # ƒê∆∞·ªùng d·∫´n model c·ªßa b·∫°n
    model_name = "Qwen3-VL-8B-Instruct"
    
    try:
        convert_to_openwebui_format(model_dir, model_name)
    except Exception as e:
        print(f"‚ùå L·ªói chuy·ªÉn ƒë·ªïi: {str(e)}")
```

## üß© H∆∞·ªõng D·∫´n S·ª≠ D·ª•ng Script

### 1. L∆∞u script v√†o file
- T·∫°o file `validate_model.py`
- T·∫°o file `convert_model.py`

### 2. C√°ch ch·∫°y
```bash
# Ki·ªÉm tra model
python validate_model.py

# Chuy·ªÉn ƒë·ªïi model
python convert_model.py
```

---

## üìå L∆∞u √ù Khi Chat tr√™n Open-WebUI

1. ƒê·∫£m b·∫£o ƒë√£ c√†i ƒë·∫∑t Open-WebUI:
   ```bash
   git clone https://github.com/open-webui/open-webui
   cd open-webui
   pip install -r requirements.txt
   ```

2. Sau khi chuy·ªÉn ƒë·ªïi, b·∫°n s·∫Ω th·∫•y file `config.json` trong folder model
3. Kh·ªüi ƒë·ªông Open-WebUI v√† ch·ªçn model t·ª´ dropdown

---

üëâ B·∫°n mu·ªën m√¨nh t·∫°o **version GUI** c·ªßa script n√†y (d√πng v·ªõi Windows/Linux) kh√¥ng?

---

**[Ph·∫ßn 1: MCP l√† g√¨](#ph·∫ßn-1)** |
**[Ph·∫ßn 2: Ki·∫øn tr√∫c gi·∫£i ph√°p](#ph·∫ßn-2)** |
**[Ph·∫ßn 3: T·∫£i, c√†i, c·∫•u h√¨nh MCP Server](#ph·∫ßn-3)** |
**[Ph·∫ßn 4: T·∫£i, c√†i, c·∫•u h√¨nh ph·∫ßn m·ªÅm MCP client](#Ph·∫ßn-4)** |
**[Ph·∫ßn 5: T√≠ch h·ª£p RAG v√† MCP Server Long Context](#ph·∫ßn-5)** |
**[Ph·∫ßn 6: Thi·∫øt k·∫ø Web API b·∫±ng AI K-AI,Mario, RAG](#ph·∫ßn-6)** |
**[Ph·∫ßn 7: Troubleshooting Debug Testing Check list:](#ph·∫ßn-7)**