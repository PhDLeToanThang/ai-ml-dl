# Pháº§n 1. Táº¡i sao AI Private Local cÃ³ hiá»‡u suáº¥t cao?

# Pháº§n 2. Trong AI Private Local mÃ´ hÃ¬nh lÃ m chá»§ Front-end vÃ  AI Agent cÃ³ AI Security an toÃ n cao?

# Pháº§n 3. APL - Ná»n táº£ng thÃ´ng tin tÃ i liá»‡u riÃªng tÆ°?

## ğŸš€ APL lÃ  gÃ¬?
APL lÃ  ná»n táº£ng Document Intelligence hoÃ n toÃ n riÃªng tÆ°, táº¡i chá»— . 
Äáº·t cÃ¢u há»i, tÃ³m táº¯t vÃ  khÃ¡m phÃ¡ thÃ´ng tin chi tiáº¿t tá»« cÃ¡c tá»‡p cá»§a báº¡n vá»›i AI tiÃªn tiáº¿nâ€”khÃ´ng cÃ³ dá»¯ liá»‡u nÃ o bá»‹ máº¥t khá»i mÃ¡y tÃ­nh cá»§a báº¡n.

- KhÃ´ng chá»‰ lÃ  **má»™t cÃ´ng cá»¥ RAG (Retrieval-Augmented Generation - Tháº¿ há»‡ TÄƒng cÆ°á»ng Truy xuáº¥t) truyá»n thá»‘ng**, 
- APL cÃ²n **sá»Ÿ há»¯u má»™t cÃ´ng cá»¥ tÃ¬m kiáº¿m lai káº¿t há»£p tÃ­nh nÄƒng tÆ°Æ¡ng Ä‘á»“ng ngá»¯ nghÄ©a (syntax Fuzzy)**, 
- khá»›p tá»« khÃ³a vÃ  **Late Chunking Ä‘á»ƒ Ä‘áº¡t Ä‘á»™ chÃ­nh xÃ¡c ngá»¯ cáº£nh dÃ i (Long Context)**. 
- Má»™t bá»™ Ä‘á»‹nh tuyáº¿n thÃ´ng minh sáº½ tá»± Ä‘á»™ng lá»±a chá»n giá»¯a RAG vÃ  LLM tráº£ lá»i trá»±c tiáº¿p cho má»i truy váº¥n (AI routing Proxy), trong khi tÃ­nh nÄƒng lÃ m giÃ u ngá»¯ cáº£nh vÃ  
 -Cáº¯t tá»‰a Ngá»¯ cáº£nh cáº¥p cÃ¢u chá»‰ hiá»ƒn thá»‹ ná»™i dung phÃ¹ há»£p nháº¥t. 
- Má»™t bÆ°á»›c xÃ¡c minh Ä‘á»™c láº­p sáº½ tÄƒng thÃªm Ä‘á»™ chÃ­nh xÃ¡c.

**Kiáº¿n trÃºc mÃ´-Ä‘un vÃ  gá»n nháº¹** â€” chá»‰ há»— trá»£ cÃ¡c thÃ nh pháº§n báº¡n cáº§n. 
- Vá»›i lÃµi Python thuáº§n tÃºy vÃ  cÃ¡c phá»¥ thuá»™c tá»‘i thiá»ƒu, APL dá»… dÃ ng triá»ƒn khai, cháº¡y vÃ  báº£o trÃ¬ trÃªn má»i cÆ¡ sá»Ÿ háº¡ táº§ng. 
- Há»‡ thá»‘ng cÃ³ Ã­t phá»¥ thuá»™c vÃ o cÃ¡c framework vÃ  thÆ° viá»‡n, giÃºp viá»‡c triá»ƒn khai vÃ  báº£o trÃ¬ dá»… dÃ ng. 
- Há»‡ thá»‘ng RAG lÃ  Python thuáº§n tÃºy vÃ  khÃ´ng yÃªu cáº§u báº¥t ká»³ phá»¥ thuá»™c bá»• sung nÃ o.

â–¶ï¸BÄƒng hÃ¬nh
Xem video (https://youtu.be/JTbtGH3secI) nÃ y Ä‘á»ƒ báº¯t Ä‘áº§u sá»­ dá»¥ng APL.

Trang chá»§	

<img width="615" height="354" alt="image" src="https://github.com/user-attachments/assets/06ec4baf-5fdd-4f66-9dc3-d43b55f9ac57" />

Táº¡o MÃ´ hÃ¬nh APL má»›i:	

<img width="640" height="933" alt="image" src="https://github.com/user-attachments/assets/1f6bef6a-df11-4f68-8ce6-6585da272fe4" />

TrÃ² chuyá»‡n

<img width="1303" height="933" alt="image" src="https://github.com/user-attachments/assets/d115b45d-be53-4034-b807-9f93f741fd99" />

## âœ¨ TÃ­nh nÄƒng
1. Quyá»n riÃªng tÆ° tá»‘i Ä‘a : Dá»¯ liá»‡u cá»§a báº¡n Ä‘Æ°á»£c lÆ°u trÃªn mÃ¡y tÃ­nh, Ä‘áº£m báº£o an toÃ n 100%.
2. Há»— trá»£ mÃ´ hÃ¬nh Ä‘a nÄƒng : TÃ­ch há»£p liá»n máº¡ch nhiá»u mÃ´ hÃ¬nh nguá»“n má»Ÿ thÃ´ng qua Ollama.
3. Nhiá»u loáº¡i nhÃºng : Chá»n tá»« nhiá»u loáº¡i nhÃºng mÃ£ nguá»“n má»Ÿ.
4. TÃ¡i sá»­ dá»¥ng LLM cá»§a báº¡n : Sau khi táº£i xuá»‘ng, báº¡n cÃ³ thá»ƒ tÃ¡i sá»­ dá»¥ng LLM mÃ  khÃ´ng cáº§n pháº£i táº£i xuá»‘ng nhiá»u láº§n.
5. Lá»‹ch sá»­ trÃ² chuyá»‡n : Ghi nhá»› cÃ¡c cuá»™c trÃ² chuyá»‡n trÆ°á»›c Ä‘Ã³ cá»§a báº¡n (trong má»™t phiÃªn).
6. API : APL cÃ³ API mÃ  báº¡n cÃ³ thá»ƒ sá»­ dá»¥ng Ä‘á»ƒ xÃ¢y dá»±ng á»¨ng dá»¥ng RAG.
7. Há»— trá»£ GPU, CPU, HPU & MPS : Há»— trá»£ nhiá»u ná»n táº£ng ngay khi cÃ i Ä‘áº·t, TrÃ² chuyá»‡n vá»›i dá»¯ liá»‡u cá»§a báº¡n báº±ng CUDA, CPU, HPU (IntelÂ® GaudiÂ®)hoáº·c MPSvÃ  nhiá»u hÆ¡n ná»¯a!
   
## ğŸ“– Xá»­ lÃ½ tÃ i liá»‡u
1. Há»— trá»£ nhiá»u Ä‘á»‹nh dáº¡ng : PDF, DOCX, TXT, Markdown vÃ  nhiá»u Ä‘á»‹nh dáº¡ng khÃ¡c (Hiá»‡n táº¡i chá»‰ há»— trá»£ PDF)
2. LÃ m giÃ u theo ngá»¯ cáº£nh : NÃ¢ng cao kháº£ nÄƒng hiá»ƒu tÃ i liá»‡u vá»›i ngá»¯ cáº£nh do AI táº¡o ra, láº¥y cáº£m há»©ng tá»« Truy xuáº¥t theo ngá»¯ cáº£nh
3. Xá»­ lÃ½ hÃ ng loáº¡t : Xá»­ lÃ½ nhiá»u tÃ i liá»‡u cÃ¹ng lÃºc
   
## ğŸ¤– TrÃ² chuyá»‡n há»— trá»£ AI
1. Truy váº¥n ngÃ´n ngá»¯ tá»± nhiÃªn : Äáº·t cÃ¢u há»i báº±ng tiáº¿ng Anh Ä‘Æ¡n giáº£n
2. Ghi rÃµ nguá»“n : Má»—i cÃ¢u tráº£ lá»i Ä‘á»u bao gá»“m tÃ i liá»‡u tham kháº£o
3. Äá»‹nh tuyáº¿n thÃ´ng minh : Tá»± Ä‘á»™ng lá»±a chá»n giá»¯a pháº£n há»“i RAG vÃ  LLM trá»±c tiáº¿p
4. PhÃ¢n tÃ­ch truy váº¥n : Chia cÃ¡c truy váº¥n phá»©c táº¡p thÃ nh cÃ¡c cÃ¢u há»i phá»¥ Ä‘á»ƒ cÃ³ cÃ¢u tráº£ lá»i tá»‘t hÆ¡n
5. Bá»™ nhá»› Ä‘á»‡m ngá»¯ nghÄ©a : Bá»™ nhá»› Ä‘á»‡m dá»±a trÃªn TTL vá»›i kháº£ nÄƒng khá»›p tÆ°Æ¡ng tá»± Ä‘á»ƒ pháº£n há»“i nhanh hÆ¡n
6. Lá»‹ch sá»­ nháº­n biáº¿t phiÃªn : Duy trÃ¬ ngá»¯ cáº£nh há»™i thoáº¡i trong suá»‘t cÃ¡c tÆ°Æ¡ng tÃ¡c
7. XÃ¡c minh cÃ¢u tráº£ lá»i : XÃ¡c minh Ä‘á»™c láº­p Ä‘á»ƒ Ä‘áº£m báº£o Ä‘á»™ chÃ­nh xÃ¡c
8. Nhiá»u mÃ´ hÃ¬nh AI : Ollama Ä‘á»ƒ suy luáº­n, HuggingFace Ä‘á»ƒ nhÃºng vÃ  xáº¿p háº¡ng láº¡i

## ğŸ› ï¸ ThÃ¢n thiá»‡n vá»›i nhÃ  phÃ¡t triá»ƒn
1. API RESTful : Truy cáº­p API hoÃ n chá»‰nh Ä‘á»ƒ tÃ­ch há»£p
2. Tiáº¿n Ä‘á»™ thá»i gian thá»±c : Cáº­p nháº­t trá»±c tiáº¿p trong quÃ¡ trÃ¬nh xá»­ lÃ½ tÃ i liá»‡u
3. Cáº¥u hÃ¬nh linh hoáº¡t : TÃ¹y chá»‰nh mÃ´ hÃ¬nh, kÃ­ch thÆ°á»›c khá»‘i vÃ  tham sá»‘ tÃ¬m kiáº¿m
4. Kiáº¿n trÃºc má»Ÿ rá»™ng : Há»‡ thá»‘ng plugin cho cÃ¡c thÃ nh pháº§n tÃ¹y chá»‰nh

## ğŸ¨ Giao diá»‡n hiá»‡n Ä‘áº¡i
1. Giao diá»‡n ngÆ°á»i dÃ¹ng web trá»±c quan : Thiáº¿t káº¿ gá»n gÃ ng, Ä‘Ã¡p á»©ng nhanh
2. Quáº£n lÃ½ phiÃªn : Tá»• chá»©c cÃ¡c cuá»™c trÃ² chuyá»‡n theo chá»§ Ä‘á»
3. Quáº£n lÃ½ chá»‰ má»¥c : Quáº£n lÃ½ bá»™ sÆ°u táº­p tÃ i liá»‡u dá»… dÃ ng
4. TrÃ² chuyá»‡n thá»i gian thá»±c : Truyá»n phÃ¡t pháº£n há»“i Ä‘á»ƒ nháº­n pháº£n há»“i ngay láº­p tá»©c

## ğŸš€ Báº¯t Ä‘áº§u nhanh
LÆ°u Ã½: Hiá»‡n táº¡i quÃ¡ trÃ¬nh cÃ i Ä‘áº·t chá»‰ Ä‘Æ°á»£c thá»­ nghiá»‡m trÃªn macOS.

## Äiá»u kiá»‡n tiÃªn quyáº¿t:
1. Python 3.8 trá»Ÿ lÃªn (Ä‘Ã£ thá»­ nghiá»‡m vá»›i Python 3.11.5)
2. Node.js 16+ vÃ  npm (Ä‘Ã£ thá»­ nghiá»‡m vá»›i Node.js 23.10.0, npm 10.9.2)
3. Docker (tÃ¹y chá»n, Ä‘á»ƒ triá»ƒn khai trong container)
4. RAM 8GB+ (khuyáº¿n nghá»‹ 16GB+)
5. Ollama (báº¯t buá»™c cho cáº£ hai cÃ¡ch triá»ƒn khai)

## GHI CHÃš:
- TrÆ°á»›c khi nhÃ¡nh nÃ y Ä‘Æ°á»£c di chuyá»ƒn Ä‘áº¿n nhÃ¡nh chÃ­nh, vui lÃ²ng sao chÃ©p nhÃ¡nh nÃ y Ä‘á»ƒ cÃ i Ä‘áº·t:
```
git clone -b APL-v2 https://github.com/PromtEngineer/localGPT.git
cd localGPT
```
## TÃ¹y chá»n 1: Triá»ƒn khai Docker
```
# Clone the repository
git clone https://github.com/PromtEngineer/localGPT.git
cd localGPT

# Install Ollama locally (required even for Docker)
curl -fsSL https://ollama.ai/install.sh | sh
ollama pull qwen3:0.6b
ollama pull qwen3:8b

# Start Ollama
ollama serve

# Start with Docker (in a new terminal)
./start-docker.sh

# Access the application
open http://localhost:3000
```
## Lá»‡nh quáº£n lÃ½ Docker:
```
# Check container status
docker compose ps

# View logs
docker compose logs -f

# Stop containers
./start-docker.sh stop
```

## Lá»±a chá»n 2: PhÃ¡t triá»ƒn trá»±c tiáº¿p (Khuyáº¿n nghá»‹ phÃ¡t triá»ƒn)
```
# Clone the repository
git clone https://github.com/PromtEngineer/localGPT.git
cd localGPT

# Install Python dependencies
pip install -r requirements.txt

# Key dependencies installed:
# - torch==2.4.1, transformers==4.51.0 (AI models)
# - lancedb (vector database)
# - rank_bm25, fuzzywuzzy (search algorithms)
# - sentence_transformers, rerankers (embedding/reranking)
# - docling (document processing)
# - colpali-engine (multimodal processing - support coming soon)

# Install Node.js dependencies
npm install

# Install and start Ollama
curl -fsSL https://ollama.ai/install.sh | sh
ollama pull qwen3:0.6b
ollama pull qwen3:8b
ollama serve

# Start the system (in a new terminal)
python run_system.py

# Access the application
open http://localhost:3000
```
## Quáº£n lÃ½ há»‡ thá»‘ng:
```
# Check system health (comprehensive diagnostics)
python system_health_check.py

# Check service status and health
python run_system.py --health

# Start in production mode
python run_system.py --mode prod

# Skip frontend (backend + RAG API only)
python run_system.py --no-frontend

# View aggregated logs
python run_system.py --logs-only

# Stop all services
python run_system.py --stop
# Or press Ctrl+C in the terminal running python run_system.py
```
## Kiáº¿n trÃºc dá»‹ch vá»¥: TrÃ¬nh run_system.pykhá»Ÿi cháº¡y quáº£n lÃ½ bá»‘n dá»‹ch vá»¥ chÃ­nh:
1. MÃ¡y chá»§ Ollama (cá»•ng 11434): Phá»¥c vá»¥ mÃ´ hÃ¬nh AI
2. MÃ¡y chá»§ API RAG (cá»•ng 8001): Xá»­ lÃ½ vÃ  truy xuáº¥t tÃ i liá»‡u
3. MÃ¡y chá»§ phá»¥ trá»£ (cá»•ng 8000): Quáº£n lÃ½ phiÃªn vÃ  Ä‘iá»ƒm cuá»‘i API
4. MÃ¡y chá»§ Frontend (cá»•ng 3000): Giao diá»‡n web React/Next.js

## TÃ¹y chá»n 3: Khá»Ÿi Ä‘á»™ng thÃ nh pháº§n thá»§ cÃ´ng
```
# Terminal 1: Start Ollama
ollama serve

# Terminal 2: Start RAG API
python -m rag_system.api_server

# Terminal 3: Start Backend
cd backend && python server.py

# Terminal 4: Start Frontend
npm run dev

# Access at http://localhost:3000
```

## CÃ i Ä‘áº·t chi tiáº¿t
1. CÃ i Ä‘áº·t cÃ¡c phá»¥ thuá»™c há»‡ thá»‘ng
Ubuntu/Debian:
```
sudo apt update
sudo apt install python3.8 python3-pip nodejs npm docker.io docker-compose
```
macOS:
```
brew install python@3.8 node npm docker docker-compose
```
Cá»­a sá»•:
```
# Install Python 3.8+, Node.js, and Docker Desktop
# Then use PowerShell or WSL2
```
2. CÃ i Ä‘áº·t mÃ´ hÃ¬nh AI
CÃ i Ä‘áº·t Ollama (Khuyáº¿n nghá»‹):
```
# Install Ollama
curl -fsSL https://ollama.ai/install.sh | sh

# Pull recommended models
ollama pull qwen3:0.6b          # Fast generation model
ollama pull qwen3:8b            # High-quality generation model
```
3. Cáº¥u hÃ¬nh mÃ´i trÆ°á»ng
```
# Copy environment template
cp .env.example .env

# Edit configuration
nano .env
```
TÃ¹y chá»n cáº¥u hÃ¬nh chÃ­nh:
```
# AI Models (referenced in rag_system/main.py)
OLLAMA_HOST=http://localhost:11434

# Database Paths (used by backend and RAG system)
DATABASE_PATH=./backend/chat_data.db
VECTOR_DB_PATH=./lancedb

# Server Settings (used by run_system.py)
BACKEND_PORT=8000
FRONTEND_PORT=3000
RAG_API_PORT=8001

# Optional: Override default models
GENERATION_MODEL=qwen3:8b
ENRICHMENT_MODEL=qwen3:0.6b
EMBEDDING_MODEL=Qwen/Qwen3-Embedding-0.6B
RERANKER_MODEL=answerdotai/answerai-colbert-small-v1
```
4. Khá»Ÿi táº¡o há»‡ thá»‘ng
```
# Run system health check
python system_health_check.py

# Initialize databases
python -c "from backend.database import ChatDatabase; ChatDatabase().init_database()"

# Test installation
python -c "from rag_system.main import get_agent; print('âœ… Installation successful!')"

# Validate complete setup
python run_system.py --health
```

---

## ğŸ¯ Báº¯t Ä‘áº§u
### 1. Táº¡o chá»‰ má»¥c Ä‘áº§u tiÃªn cá»§a báº¡n
Chá»‰ má»¥c lÃ  táº­p há»£p cÃ¡c tÃ i liá»‡u Ä‘Ã£ xá»­ lÃ½ mÃ  báº¡n cÃ³ thá»ƒ trÃ² chuyá»‡n.

Sá»­ dá»¥ng Giao diá»‡n Web:
1. Má»Ÿ http://localhost:3000
2. Nháº¥p vÃ o "Táº¡o chá»‰ má»¥c má»›i"
3. Táº£i lÃªn tÃ i liá»‡u cá»§a báº¡n (PDF, DOCX, TXT)
4. Cáº¥u hÃ¬nh tÃ¹y chá»n xá»­ lÃ½
5. Nháº¥p vÃ o "XÃ¢y dá»±ng chá»‰ má»¥c"
Sá»­ dá»¥ng táº­p lá»‡nh:
```
# Simple script approach
./simple_create_index.sh "My Documents" "path/to/document.pdf"

# Interactive script
python create_index_script.py
```
Sá»­ dá»¥ng API:
```
# Create index
curl -X POST http://localhost:8000/indexes \
  -H "Content-Type: application/json" \
  -d '{"name": "My Index", "description": "My documents"}'

# Upload documents
curl -X POST http://localhost:8000/indexes/INDEX_ID/upload \
  -F "files=@document.pdf"

# Build index
curl -X POST http://localhost:8000/indexes/INDEX_ID/build
```
### 2. Báº¯t Ä‘áº§u trÃ² chuyá»‡n
Sau khi chá»‰ má»¥c cá»§a báº¡n Ä‘Æ°á»£c xÃ¢y dá»±ng:
1. Táº¡o phiÃªn trÃ² chuyá»‡n : Nháº¥p vÃ o "TrÃ² chuyá»‡n má»›i" hoáº·c sá»­ dá»¥ng phiÃªn trÃ² chuyá»‡n hiá»‡n cÃ³
2. Chá»n chá»‰ má»¥c cá»§a báº¡n : Chá»n bá»™ sÆ°u táº­p tÃ i liá»‡u Ä‘á»ƒ truy váº¥n
3. Äáº·t cÃ¢u há»i : Nháº­p cÃ¢u há»i báº±ng ngÃ´n ngá»¯ tá»± nhiÃªn vá» tÃ i liá»‡u cá»§a báº¡n
4. Nháº­n cÃ¢u tráº£ lá»i : Nháº­n pháº£n há»“i do AI táº¡o ra kÃ¨m theo trÃ­ch dáº«n nguá»“n

### 3. TÃ­nh nÄƒng nÃ¢ng cao
Cáº¥u hÃ¬nh mÃ´ hÃ¬nh tÃ¹y chá»‰nh:
```
# Use different models for different tasks
curl -X POST http://localhost:8000/sessions \
  -H "Content-Type: application/json" \
  -d '{
    "title": "High Quality Session",
    "model": "qwen3:8b",
    "embedding_model": "Qwen/Qwen3-Embedding-4B"
  }'
```
Xá»­ lÃ½ tÃ i liá»‡u hÃ ng loáº¡t
```
# Process multiple documents at once
python demo_batch_indexing.py --config batch_indexing_config.json
```

TÃ­ch há»£p API:
```
import requests

# Chat with your documents via API
response = requests.post('http://localhost:8000/chat', json={
    'query': 'What are the key findings in the research papers?',
    'session_id': 'your-session-id',
    'search_type': 'hybrid',
    'retrieval_k': 20
})

print(response.json()['response'])
```
## ğŸ”§ Cáº¥u hÃ¬nh
### Cáº¥u hÃ¬nh mÃ´ hÃ¬nh

APL há»— trá»£ nhiá»u nhÃ  cung cáº¥p mÃ´ hÃ¬nh AI vá»›i cáº¥u hÃ¬nh táº­p trung:
#### 1. MÃ´ hÃ¬nh Ollama (Suy luáº­n cá»¥c bá»™)
```
OLLAMA_CONFIG = {
    "host": "http://localhost:11434",
    "generation_model": "qwen3:8b",        # Main text generation
    "enrichment_model": "qwen3:0.6b"       # Lightweight routing/enrichment
}
```
#### MÃ´ hÃ¬nh máº«u bÃªn ngoÃ i (HuggingFace Direct)
```
EXTERNAL_MODELS = {
    "embedding_model": "Qwen/Qwen3-Embedding-0.6B",           # 1024 dimensions
    "reranker_model": "answerdotai/answerai-colbert-small-v1", # ColBERT reranker
    "fallback_reranker": "BAAI/bge-reranker-base"             # Backup reranker
}
```
#### Cáº¥u hÃ¬nh Ä‘Æ°á»ng á»‘ng:
APL cung cáº¥p hai cáº¥u hÃ¬nh Ä‘Æ°á»ng á»‘ng chÃ­nh:

ÄÆ°á»ng á»‘ng máº·c Ä‘á»‹nh (Sáºµn sÃ ng sáº£n xuáº¥t)
```
"default": {
    "description": "Production-ready pipeline with hybrid search, AI reranking, and verification",
    "storage": {
        "lancedb_uri": "./lancedb",
        "text_table_name": "text_pages_v3",
        "bm25_path": "./index_store/bm25"
    },
    "retrieval": {
        "retriever": "multivector",
        "search_type": "hybrid",
        "late_chunking": {"enabled": True},
        "dense": {"enabled": True, "weight": 0.7},
        "bm25": {"enabled": True}
    },
    "reranker": {
        "enabled": True,
        "type": "ai",
        "strategy": "rerankers-lib",
        "model_name": "answerdotai/answerai-colbert-small-v1",
        "top_k": 10
    },
    "query_decomposition": {"enabled": True, "max_sub_queries": 3},
    "verification": {"enabled": True},
    "retrieval_k": 20,
    "contextual_enricher": {"enabled": True, "window_size": 1}
}
```
#### ÄÆ°á»ng á»‘ng nhanh (Tá»‘i Æ°u hÃ³a tá»‘c Ä‘á»™)
```
"fast": {
    "description": "Speed-optimized pipeline with minimal overhead",
    "retrieval": {
        "search_type": "vector_only",
        "late_chunking": {"enabled": False}
    },
    "reranker": {"enabled": False},
    "query_decomposition": {"enabled": False},
    "verification": {"enabled": False},
    "retrieval_k": 10,
    "contextual_enricher": {"enabled": False}
}
```
#### Cáº¥u hÃ¬nh tÃ¬m kiáº¿m:
```
SEARCH_CONFIG = {
    'hybrid': {
        'dense_weight': 0.7,
        'sparse_weight': 0.3,
        'retrieval_k': 20,
        'reranker_top_k': 10
    }
}
```
## ğŸ› ï¸ Kháº¯c phá»¥c sá»± cá»‘:
### CÃ¡c váº¥n Ä‘á» chung:
#### Sá»± cá»‘ cÃ i Ä‘áº·t:
```
# Check Python version
python --version  # Should be 3.8+

# Check dependencies
pip list | grep -E "(torch|transformers|lancedb)"

# Reinstall dependencies
pip install -r requirements.txt --force-reinstall
```
#### CÃ¡c váº¥n Ä‘á» táº£i mÃ´ hÃ¬nh
```
# Check Ollama status
ollama list
curl http://localhost:11434/api/tags

# Pull missing models
ollama pull qwen3:0.6b
```
#### CÃ¡c váº¥n Ä‘á» vá» cÆ¡ sá»Ÿ dá»¯ liá»‡u
```
# Check database connectivity
python -c "from backend.database import ChatDatabase; db = ChatDatabase(); print('âœ… Database OK')"

# Reset database (WARNING: This deletes all data)
rm backend/chat_data.db
python -c "from backend.database import ChatDatabase; ChatDatabase().init_database()"
```
#### CÃ¡c váº¥n Ä‘á» vá» hiá»‡u suáº¥t
```
# Check system resources
python system_health_check.py

# Monitor memory usage
htop  # or Task Manager on Windows

# Optimize for low-memory systems
export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512
```

### Nháº­n trá»£ giÃºp
**1. Kiá»ƒm tra Nháº­t kÃ½** : Há»‡ thá»‘ng táº¡o nháº­t kÃ½ cÃ³ cáº¥u trÃºc trong logs/thÆ° má»¥c:

- logs/system.log: CÃ¡c sá»± kiá»‡n vÃ  lá»—i há»‡ thá»‘ng chÃ­nh
- logs/ollama.log: Nháº­t kÃ½ mÃ¡y chá»§ Ollama
- logs/rag-api.log: Nháº­t kÃ½ xá»­ lÃ½ API RAG
- logs/backend.log: Nháº­t kÃ½ mÃ¡y chá»§ phá»¥ trá»£
- logs/frontend.log: Nháº­t kÃ½ xÃ¢y dá»±ng vÃ  thá»i gian cháº¡y giao diá»‡n ngÆ°á»i dÃ¹ng

**2. Sá»©c khá»e há»‡ thá»‘ng**: Cháº¡y cháº©n Ä‘oÃ¡n toÃ n diá»‡n:
```
python system_health_check.py  # Full system diagnostics
python run_system.py --health  # Service status check
```
**3. Äiá»ƒm cuá»‘i vá» tÃ¬nh tráº¡ng sá»©c khá»e :** Kiá»ƒm tra tÃ¬nh tráº¡ng sá»©c khá»e cá»§a tá»«ng dá»‹ch vá»¥:

- Pháº§n cuá»‘i:http://localhost:8000/health
- API RAG:http://localhost:8001/health
- Ollama:http://localhost:11434/api/tags

**4. TÃ i liá»‡u : Kiá»ƒm tra TÃ i liá»‡u Ká»¹ thuáº­t**: https://github.com/PromtEngineer/localGPT/blob/main/TECHNICAL_DOCS.md 

**5. Sá»± cá»‘ GitHub :** BÃ¡o cÃ¡o lá»—i vÃ  yÃªu cáº§u tÃ­nh nÄƒng

**6. Cá»™ng Ä‘á»“ng :** Tham gia cá»™ng Ä‘á»“ng Discord/Slack cá»§a chÃºng tÃ´i

---

# ğŸ”— TÃ i liá»‡u tham kháº£o API
## Äiá»ƒm cuá»‘i cá»‘t lÃµi
### API trÃ² chuyá»‡n
```
# Session-based chat (recommended)
POST /sessions/{session_id}/chat
Content-Type: application/json

{
  "query": "What are the main topics discussed?",
  "search_type": "hybrid",
  "retrieval_k": 20,
  "ai_rerank": true,
  "context_window_size": 5
}

# Legacy chat endpoint
POST /chat
Content-Type: application/json

{
  "query": "What are the main topics discussed?",
  "session_id": "uuid",
  "search_type": "hybrid",
  "retrieval_k": 20
}
```
### Quáº£n lÃ½ chá»‰ má»¥c:
```
# Create index
POST /indexes
Content-Type: application/json
{
  "name": "My Index",
  "description": "Description",
  "config": "default"
}

# Get all indexes
GET /indexes

# Get specific index
GET /indexes/{id}

# Upload documents to index
POST /indexes/{id}/upload
Content-Type: multipart/form-data
files: [file1.pdf, file2.pdf, ...]

# Build index (process uploaded documents)
POST /indexes/{id}/build
Content-Type: application/json
{
  "config_mode": "default",
  "enable_enrich": true,
  "chunk_size": 512
}

# Delete index
DELETE /indexes/{id}
```
### Quáº£n lÃ½ phiÃªn:
```
# Create session
POST /sessions
Content-Type: application/json
{
  "title": "My Session",
  "model": "qwen3:0.6b"
}

# Get all sessions
GET /sessions

# Get specific session
GET /sessions/{session_id}

# Get session documents
GET /sessions/{session_id}/documents

# Get session indexes
GET /sessions/{session_id}/indexes

# Link index to session
POST /sessions/{session_id}/indexes/{index_id}

# Delete session
DELETE /sessions/{session_id}

# Rename session
POST /sessions/{session_id}/rename
Content-Type: application/json
{
  "new_title": "Updated Session Name"
}
```

### TÃ­nh nÄƒng nÃ¢ng cao
#### PhÃ¢n tÃ­ch truy váº¥n
Há»‡ thá»‘ng cÃ³ thá»ƒ chia cÃ¡c truy váº¥n phá»©c táº¡p thÃ nh cÃ¡c cÃ¢u há»i nhá» hÆ¡n Ä‘á»ƒ cÃ³ cÃ¢u tráº£ lá»i tá»‘t hÆ¡n:
```
POST /sessions/{session_id}/chat
Content-Type: application/json

{
  "query": "Compare the methodologies and analyze their effectiveness",
  "query_decompose": true,
  "compose_sub_answers": true
}
```
#### XÃ¡c minh cÃ¢u tráº£ lá»i
XÃ¡c minh Ä‘á»™c láº­p vá» Ä‘á»™ chÃ­nh xÃ¡c báº±ng cÃ¡ch sá»­ dá»¥ng mÃ´ hÃ¬nh xÃ¡c minh riÃªng biá»‡t:
```
POST /sessions/{session_id}/chat
Content-Type: application/json

{
  "query": "What are the key findings?",
  "verify": true
}
```
#### LÃ m giÃ u ngá»¯ cáº£nh
LÃ m giÃ u ngá»¯ cáº£nh tÃ i liá»‡u trong quÃ¡ trÃ¬nh láº­p chá»‰ má»¥c Ä‘á»ƒ hiá»ƒu rÃµ hÆ¡n:
```
# Enable during index building
POST /indexes/{id}/build
{
  "enable_enrich": true,
  "window_size": 2
}
```
#### PhÃ¢n Ä‘oáº¡n muá»™n
Báº£o toÃ n ngá»¯ cáº£nh tá»‘t hÆ¡n báº±ng cÃ¡ch phÃ¢n Ä‘oáº¡n sau khi nhÃºng:
```
# Configure in pipeline
"late_chunking": {"enabled": true}
```
#### TrÃ² chuyá»‡n trá»±c tuyáº¿n
```
POST /chat/stream
Content-Type: application/json

{
  "query": "Explain the methodology",
  "session_id": "uuid",
  "stream": true
}
```
#### Xá»­ lÃ½ hÃ ng loáº¡t
```
# Using the batch indexing script
python demo_batch_indexing.py --config batch_indexing_config.json

# Example batch configuration (batch_indexing_config.json):
{
  "index_name": "Sample Batch Index",
  "index_description": "Example batch index configuration",
  "documents": [
    "./rag_system/documents/invoice_1039.pdf",
    "./rag_system/documents/invoice_1041.pdf"
  ],
  "processing": {
    "chunk_size": 512,
    "chunk_overlap": 64,
    "enable_enrich": true,
    "enable_latechunk": true,
    "enable_docling": true,
    "embedding_model": "Qwen/Qwen3-Embedding-0.6B",
    "generation_model": "qwen3:0.6b",
    "retrieval_mode": "hybrid",
    "window_size": 2
  }
}
```
```
# API endpoint for batch processing
POST /batch/index
Content-Type: application/json

{
  "file_paths": ["doc1.pdf", "doc2.pdf"],
  "config": {
    "chunk_size": 512,
    "enable_enrich": true,
    "enable_latechunk": true,
    "enable_docling": true
  }
}
```
Äá»ƒ biáº¿t tÃ i liá»‡u API Ä‘áº§y Ä‘á»§, hÃ£y xem API_REFERENCE.md : https://github.com/PromtEngineer/localGPT/blob/main/API_REFERENCE.md.

# ğŸ—ï¸ Kiáº¿n trÃºc
APL Ä‘Æ°á»£c xÃ¢y dá»±ng theo kiáº¿n trÃºc mÃ´-Ä‘un, cÃ³ kháº£ nÄƒng má»Ÿ rá»™ng:

```mermaid
graph TB
    UI[Giao diá»‡n web] --> API[API Database]
    API --> Agent[Äáº¡i lÃ½ RAG]
    Agent --> Retrieval[Retrieval Pipeline]
    Agent --> Generation[Generation Pipeline]

    Retrieval --> Vector[TÃ¬m kiáº¿m Vector]
    Retrieval --> BM25[TÃ¬m kiáº¿m BM25]
    Retrieval --> Rerank[Xáº¿p háº¡ng láº¡i]

    Vector --> LanceDB[(LanceDB)]
    BM25 --> BM25DB[(Má»¥c lá»¥c BM25)]

    Generation --> Ollama[MÃ´ hÃ¬nh Ollama]
    Generation --> HF[MÃ´ hÃ¬nh trÃªn HGF]

    API --> SQLite[(CÆ¡ sá»Ÿ dá»¯ liá»‡u SQLite)]
```
Tá»•ng quan vá» tÃ¡c nhÃ¢n truy xuáº¥t

```mermaid
graph TB
    classDef llmcall fill:#e6f3ff,stroke:#007bff;
    classDef pipeline fill:#e6ffe6,stroke:#28a745;
    classDef cache fill:#fff3e0,stroke:#fd7e14;
    classDef logic fill:#f8f9fa,stroke:#6c757d;
    classDef thread stroke-dasharray: 5 5;

    A(Báº¯t Ä‘áº§u: Agent.run) --> B_asyncio.run(_run_async);
    B --> C{_run_async};

    C --> C1[Láº¥y lá»‹ch sá»­ trÃ² chuyá»‡n];
    C1 --> T1[XÃ¢y dá»±ng lá»i nháº¯c phÃ¢n loáº¡i <br/> Truy váº¥n + Tá»•ng quan tÃ i liá»‡u];
    T1 --> T2["(asyncio.to_thread)<br/>PhÃ¢n loáº¡i LLM: RAG hay LLM_DIRECT?"]; class T2 llmcall,thread;
    T2 --> T3{Quyáº¿t Ä‘á»‹nh?};

    T3 -- RAG --> RAG_Path;
    T3 -- LLM_DIRECT --> LLM_Path;

    subgraph RAG Path
        RAG_Path --> R1["Äá»‹nh dáº¡ng truy váº¥n + Lá»‹ch sá»­"];
        R1 --> R2["(asyncio.to_thread)<br/>Táº¡o nhÃºng truy váº¥n"]; class R2 pipeline,thread;
        R2 --> R3{{Kiá»ƒm tra bá»™ Ä‘á»‡m ngá»¯ nghÄ©a}};  class R3 cache;
        R3 -- Hit --> R_Cache_Hit("Tráº£ vá» káº¿t quáº£ trong bá»™ nhá»› Ä‘á»‡m");
        R_Cache_Hit --> R_Hist_Update;
        R3 -- Miss --> R4{PhÃ¢n há»§y <br/> ÄÃ£ báº­t?};

        R4 -- Yes --> R5["(asyncio.to_thread)<br/>PhÃ¢n tÃ­ch truy váº¥n thÃ´"]; class R5 llmcall,thread;
        R5 --> R6{{Cháº¡y truy váº¥n phá»¥ <br/> ÄÆ°á»ng á»‘ng RAG song song}}; class R6 pipeline,thread;
        R6 --> R7[Thu tháº­p káº¿t quáº£ vÃ  tÃ i liá»‡u];
        R7 --> R8["(asyncio.to_thread)<br/>Soáº¡n cÃ¢u tráº£ lá»i cuá»‘i cÃ¹ng"]; class R8 llmcall,thread;
        R8 --> V1(CÃ¢u tráº£ lá»i cá»§a RAG);

        R4 -- No --> R9["(asyncio.to_thread)<br/>Cháº¡y truy váº¥n Ä‘Æ¡n <br/>(RAG Pipeline)"]; class R9 pipeline,thread;
        R9 --> V1;

        V1 --> V2{{XÃ¡c minh <br/> await verify_async}}; class V2 llmcall;
        V2 --> V3("Káº¿t quáº£ RAG cuá»‘i cÃ¹ng");
        V3 --> R_Cache_Store{{LÆ°u trá»¯ trong bá»™ nhá»› Ä‘á»‡m ngá»¯ nghÄ©a}}; class R_Cache_Store cache;
        R_Cache_Store --> FinalResult;
    end

    subgraph Direct LLM Path
        LLM_Path --> L1["Äá»‹nh dáº¡ng truy váº¥n + Lá»‹ch sá»­"];
        L1 --> L2["(asyncio.to_thread)<br/>Táº¡o cÃ¢u tráº£ lá»i LLM trá»±c tiáº¿p <br/> (KhÃ´ng cÃ³ RAG)"]; class L2 llmcall,thread;
        L2 --> FinalResult("Káº¿t quáº£ trá»±c tiáº¿p cuá»‘i cÃ¹ng");
    end

    FinalResult --> R_Hist_Update("Cáº­p nháº­t lá»‹ch sá»­ trÃ² chuyá»‡n");
    R_Hist_Update --> ZZZ("Káº¿t thÃºc: Tráº£ vá» káº¿t quáº£");
```

# ğŸ¤ ÄÃ³ng gÃ³p:
ChÃºng tÃ´i hoan nghÃªnh sá»± Ä‘Ã³ng gÃ³p tá»« cÃ¡c nhÃ  phÃ¡t triá»ƒn á»Ÿ má»i trÃ¬nh Ä‘á»™! APL lÃ  má»™t dá»± Ã¡n nguá»“n má»Ÿ Ä‘Æ°á»£c hÆ°á»Ÿng lá»£i tá»« sá»± tham gia cá»§a cá»™ng Ä‘á»“ng.

## ğŸš€ Báº¯t Ä‘áº§u nhanh cho NgÆ°á»i Ä‘Ã³ng gÃ³p
```
# Fork and clone the repository
git clone https://github.com/PromtEngineer/APL.git
cd APL

# Set up development environment
pip install -r requirements.txt
npm install

# Install Ollama and models
curl -fsSL https://ollama.ai/install.sh | sh
ollama pull qwen3:0.6b qwen3:8b

# Verify setup
python system_health_check.py
python run_system.py --mode dev
```
### ğŸ“‹ CÃ¡ch Ä‘Ã³ng gÃ³p
1. ğŸ› BÃ¡o cÃ¡o lá»—i : Sá»­dá»¥ng máº«u bÃ¡o cÃ¡o lá»—i cá»§a chÃºng tÃ´i https://github.com/PromtEngineer/localGPT/blob/main/.github/ISSUE_TEMPLATE/bug_report.md
2. ğŸ’¡ YÃªu cáº§u tÃ­nh nÄƒng : Sá»­ dá»¥ng máº«u yÃªu cáº§u tÃ­nh nÄƒng cá»§a chÃºng tÃ´i https://github.com/PromtEngineer/localGPT/blob/main/.github/ISSUE_TEMPLATE/feature_request.md
3. ğŸ”§ Gá»­i mÃ£ : Theo dÃµi quy trÃ¬nh phÃ¡t triá»ƒn cá»§a chÃºng tÃ´i: https://github.com/PromtEngineer/localGPT/blob/main/CONTRIBUTING.md#development-workflow
4. ğŸ“š Cáº£i thiá»‡n TÃ i liá»‡u : GiÃºp cáº£i thiá»‡n tÃ i liá»‡u cá»§a chÃºng tÃ´i

### ğŸ“– HÆ°á»›ng dáº«n chi tiáº¿t
Äá»ƒ biáº¿t hÆ°á»›ng dáº«n Ä‘Ã³ng gÃ³p toÃ n diá»‡n, bao gá»“m:

1. Thiáº¿t láº­p phÃ¡t triá»ƒn vÃ  quy trÃ¬nh lÃ m viá»‡c
2. TiÃªu chuáº©n mÃ£ hÃ³a vÃ  thá»±c hÃ nh tá»‘t nháº¥t
3. YÃªu cáº§u thá»­ nghiá»‡m
4. TiÃªu chuáº©n tÃ i liá»‡u
5. QuÃ¡ trÃ¬nh phÃ¡t hÃ nh
ğŸ‘‰ Xem hÆ°á»›ng dáº«n CONTRIBUTING.md cá»§a chÃºng tÃ´i: https://github.com/PromtEngineer/localGPT/blob/main/CONTRIBUTING.md

### ğŸ“„ Giáº¥y phÃ©p
Dá»± Ã¡n nÃ y Ä‘Æ°á»£c cáº¥p phÃ©p theo Giáº¥y phÃ©p MIT - xem tá»‡p LICENSE https://github.com/PromtEngineer/localGPT/blob/main/LICENSE Ä‘á»ƒ biáº¿t chi tiáº¿t. 
Äá»‘i vá»›i cÃ¡c mÃ´ hÃ¬nh, vui lÃ²ng kiá»ƒm tra giáº¥y phÃ©p tÆ°Æ¡ng á»©ng.

### ğŸ“ Há»— trá»£
1. TÃ i liá»‡u : TÃ i liá»‡u ká»¹ thuáº­t: https://github.com/PromtEngineer/localGPT/blob/main/TECHNICAL_DOCS.md
2. CÃ¡c váº¥n Ä‘á» : CÃ¡c váº¥n Ä‘á» cá»§a GitHub: https://github.com/PromtEngineer/localGPT/issues
3. Tháº£o luáº­n : Tháº£o luáº­n GitHub: https://github.com/PromtEngineer/localGPT/discussions
4. Triá»ƒn khai vÃ  tÃ¹y chá»‰nh doanh nghiá»‡p : LiÃªn há»‡ vá»›i chÃºng tÃ´i: https://tally.so/r/wv6R2d
#### Lá»‹ch sá»­ ngÃ´i sao: star-history.com 
