# Ph·∫ßn 1. T·∫°i sao AI Private Local c√≥ hi·ªáu su·∫•t cao?

# Ph·∫ßn 2. Trong AI Private Local m√¥ h√¨nh l√†m ch·ªß Front-end v√† AI Agent c√≥ AI Security an to√†n cao?

# Ph·∫ßn 3. APL - N·ªÅn t·∫£ng th√¥ng tin t√†i li·ªáu ri√™ng t∆∞?

## üöÄ APL l√† g√¨?
APL l√† n·ªÅn t·∫£ng Document Intelligence ho√†n to√†n ri√™ng t∆∞, t·∫°i ch·ªó . 
ƒê·∫∑t c√¢u h·ªèi, t√≥m t·∫Øt v√† kh√°m ph√° th√¥ng tin chi ti·∫øt t·ª´ c√°c t·ªáp c·ªßa b·∫°n v·ªõi AI ti√™n ti·∫øn‚Äîkh√¥ng c√≥ d·ªØ li·ªáu n√†o b·ªã m·∫•t kh·ªèi m√°y t√≠nh c·ªßa b·∫°n.

- Kh√¥ng ch·ªâ l√† **m·ªôt c√¥ng c·ª• RAG (Retrieval-Augmented Generation - Th·∫ø h·ªá TƒÉng c∆∞·ªùng Truy xu·∫•t) truy·ªÅn th·ªëng**, 
- APL c√≤n **s·ªü h·ªØu m·ªôt c√¥ng c·ª• t√¨m ki·∫øm lai k·∫øt h·ª£p t√≠nh nƒÉng t∆∞∆°ng ƒë·ªìng ng·ªØ nghƒ©a (syntax Fuzzy)**, 
- kh·ªõp t·ª´ kh√≥a v√† **Late Chunking ƒë·ªÉ ƒë·∫°t ƒë·ªô ch√≠nh x√°c ng·ªØ c·∫£nh d√†i (Long Context)**. 
- M·ªôt b·ªô ƒë·ªãnh tuy·∫øn th√¥ng minh s·∫Ω t·ª± ƒë·ªông l·ª±a ch·ªçn gi·ªØa RAG v√† LLM tr·∫£ l·ªùi tr·ª±c ti·∫øp cho m·ªçi truy v·∫•n (AI routing Proxy), trong khi t√≠nh nƒÉng l√†m gi√†u ng·ªØ c·∫£nh v√† 
 -C·∫Øt t·ªâa Ng·ªØ c·∫£nh c·∫•p c√¢u ch·ªâ hi·ªÉn th·ªã n·ªôi dung ph√π h·ª£p nh·∫•t. 
- M·ªôt b∆∞·ªõc x√°c minh ƒë·ªôc l·∫≠p s·∫Ω tƒÉng th√™m ƒë·ªô ch√≠nh x√°c.

**Ki·∫øn tr√∫c m√¥-ƒëun v√† g·ªçn nh·∫π** ‚Äî ch·ªâ h·ªó tr·ª£ c√°c th√†nh ph·∫ßn b·∫°n c·∫ßn. 
- V·ªõi l√µi Python thu·∫ßn t√∫y v√† c√°c ph·ª• thu·ªôc t·ªëi thi·ªÉu, APL d·ªÖ d√†ng tri·ªÉn khai, ch·∫°y v√† b·∫£o tr√¨ tr√™n m·ªçi c∆° s·ªü h·∫° t·∫ßng. 
- H·ªá th·ªëng c√≥ √≠t ph·ª• thu·ªôc v√†o c√°c framework v√† th∆∞ vi·ªán, gi√∫p vi·ªác tri·ªÉn khai v√† b·∫£o tr√¨ d·ªÖ d√†ng. 
- H·ªá th·ªëng RAG l√† Python thu·∫ßn t√∫y v√† kh√¥ng y√™u c·∫ßu b·∫•t k·ª≥ ph·ª• thu·ªôc b·ªï sung n√†o.

‚ñ∂Ô∏èBƒÉng h√¨nh
Xem video (https://youtu.be/JTbtGH3secI) n√†y ƒë·ªÉ b·∫Øt ƒë·∫ßu s·ª≠ d·ª•ng APL.

Trang ch·ªß	

<img width="615" height="354" alt="image" src="https://github.com/user-attachments/assets/06ec4baf-5fdd-4f66-9dc3-d43b55f9ac57" />

T·∫°o M√¥ h√¨nh APL m·ªõi:	

<img width="640" height="933" alt="image" src="https://github.com/user-attachments/assets/1f6bef6a-df11-4f68-8ce6-6585da272fe4" />

Tr√≤ chuy·ªán

<img width="1303" height="933" alt="image" src="https://github.com/user-attachments/assets/d115b45d-be53-4034-b807-9f93f741fd99" />

## ‚ú® T√≠nh nƒÉng
1. Quy·ªÅn ri√™ng t∆∞ t·ªëi ƒëa : D·ªØ li·ªáu c·ªßa b·∫°n ƒë∆∞·ª£c l∆∞u tr√™n m√°y t√≠nh, ƒë·∫£m b·∫£o an to√†n 100%.
2. H·ªó tr·ª£ m√¥ h√¨nh ƒëa nƒÉng : T√≠ch h·ª£p li·ªÅn m·∫°ch nhi·ªÅu m√¥ h√¨nh ngu·ªìn m·ªü th√¥ng qua Ollama.
3. Nhi·ªÅu lo·∫°i nh√∫ng : Ch·ªçn t·ª´ nhi·ªÅu lo·∫°i nh√∫ng m√£ ngu·ªìn m·ªü.
4. T√°i s·ª≠ d·ª•ng LLM c·ªßa b·∫°n : Sau khi t·∫£i xu·ªëng, b·∫°n c√≥ th·ªÉ t√°i s·ª≠ d·ª•ng LLM m√† kh√¥ng c·∫ßn ph·∫£i t·∫£i xu·ªëng nhi·ªÅu l·∫ßn.
5. L·ªãch s·ª≠ tr√≤ chuy·ªán : Ghi nh·ªõ c√°c cu·ªôc tr√≤ chuy·ªán tr∆∞·ªõc ƒë√≥ c·ªßa b·∫°n (trong m·ªôt phi√™n).
6. API : APL c√≥ API m√† b·∫°n c√≥ th·ªÉ s·ª≠ d·ª•ng ƒë·ªÉ x√¢y d·ª±ng ·ª®ng d·ª•ng RAG.
7. H·ªó tr·ª£ GPU, CPU, HPU & MPS : H·ªó tr·ª£ nhi·ªÅu n·ªÅn t·∫£ng ngay khi c√†i ƒë·∫∑t, Tr√≤ chuy·ªán v·ªõi d·ªØ li·ªáu c·ªßa b·∫°n b·∫±ng CUDA, CPU, HPU (Intel¬Æ Gaudi¬Æ)ho·∫∑c MPSv√† nhi·ªÅu h∆°n n·ªØa!
   
## üìñ X·ª≠ l√Ω t√†i li·ªáu
1. H·ªó tr·ª£ nhi·ªÅu ƒë·ªãnh d·∫°ng : PDF, DOCX, TXT, Markdown v√† nhi·ªÅu ƒë·ªãnh d·∫°ng kh√°c (Hi·ªán t·∫°i ch·ªâ h·ªó tr·ª£ PDF)
2. L√†m gi√†u theo ng·ªØ c·∫£nh : N√¢ng cao kh·∫£ nƒÉng hi·ªÉu t√†i li·ªáu v·ªõi ng·ªØ c·∫£nh do AI t·∫°o ra, l·∫•y c·∫£m h·ª©ng t·ª´ Truy xu·∫•t theo ng·ªØ c·∫£nh
3. X·ª≠ l√Ω h√†ng lo·∫°t : X·ª≠ l√Ω nhi·ªÅu t√†i li·ªáu c√πng l√∫c
   
## ü§ñ Tr√≤ chuy·ªán h·ªó tr·ª£ AI
1. Truy v·∫•n ng√¥n ng·ªØ t·ª± nhi√™n : ƒê·∫∑t c√¢u h·ªèi b·∫±ng ti·∫øng Anh ƒë∆°n gi·∫£n
2. Ghi r√µ ngu·ªìn : M·ªói c√¢u tr·∫£ l·ªùi ƒë·ªÅu bao g·ªìm t√†i li·ªáu tham kh·∫£o
3. ƒê·ªãnh tuy·∫øn th√¥ng minh : T·ª± ƒë·ªông l·ª±a ch·ªçn gi·ªØa ph·∫£n h·ªìi RAG v√† LLM tr·ª±c ti·∫øp
4. Ph√¢n t√≠ch truy v·∫•n : Chia c√°c truy v·∫•n ph·ª©c t·∫°p th√†nh c√°c c√¢u h·ªèi ph·ª• ƒë·ªÉ c√≥ c√¢u tr·∫£ l·ªùi t·ªët h∆°n
5. B·ªô nh·ªõ ƒë·ªám ng·ªØ nghƒ©a : B·ªô nh·ªõ ƒë·ªám d·ª±a tr√™n TTL v·ªõi kh·∫£ nƒÉng kh·ªõp t∆∞∆°ng t·ª± ƒë·ªÉ ph·∫£n h·ªìi nhanh h∆°n
6. L·ªãch s·ª≠ nh·∫≠n bi·∫øt phi√™n : Duy tr√¨ ng·ªØ c·∫£nh h·ªôi tho·∫°i trong su·ªët c√°c t∆∞∆°ng t√°c
7. X√°c minh c√¢u tr·∫£ l·ªùi : X√°c minh ƒë·ªôc l·∫≠p ƒë·ªÉ ƒë·∫£m b·∫£o ƒë·ªô ch√≠nh x√°c
8. Nhi·ªÅu m√¥ h√¨nh AI : Ollama ƒë·ªÉ suy lu·∫≠n, HuggingFace ƒë·ªÉ nh√∫ng v√† x·∫øp h·∫°ng l·∫°i

## üõ†Ô∏è Th√¢n thi·ªán v·ªõi nh√† ph√°t tri·ªÉn
1. API RESTful : Truy c·∫≠p API ho√†n ch·ªânh ƒë·ªÉ t√≠ch h·ª£p
2. Ti·∫øn ƒë·ªô th·ªùi gian th·ª±c : C·∫≠p nh·∫≠t tr·ª±c ti·∫øp trong qu√° tr√¨nh x·ª≠ l√Ω t√†i li·ªáu
3. C·∫•u h√¨nh linh ho·∫°t : T√πy ch·ªânh m√¥ h√¨nh, k√≠ch th∆∞·ªõc kh·ªëi v√† tham s·ªë t√¨m ki·∫øm
4. Ki·∫øn tr√∫c m·ªü r·ªông : H·ªá th·ªëng plugin cho c√°c th√†nh ph·∫ßn t√πy ch·ªânh

## üé® Giao di·ªán hi·ªán ƒë·∫°i
1. Giao di·ªán ng∆∞·ªùi d√πng web tr·ª±c quan : Thi·∫øt k·∫ø g·ªçn g√†ng, ƒë√°p ·ª©ng nhanh
2. Qu·∫£n l√Ω phi√™n : T·ªï ch·ª©c c√°c cu·ªôc tr√≤ chuy·ªán theo ch·ªß ƒë·ªÅ
3. Qu·∫£n l√Ω ch·ªâ m·ª•c : Qu·∫£n l√Ω b·ªô s∆∞u t·∫≠p t√†i li·ªáu d·ªÖ d√†ng
4. Tr√≤ chuy·ªán th·ªùi gian th·ª±c : Truy·ªÅn ph√°t ph·∫£n h·ªìi ƒë·ªÉ nh·∫≠n ph·∫£n h·ªìi ngay l·∫≠p t·ª©c

## üöÄ B·∫Øt ƒë·∫ßu nhanh
L∆∞u √Ω: Hi·ªán t·∫°i qu√° tr√¨nh c√†i ƒë·∫∑t ch·ªâ ƒë∆∞·ª£c th·ª≠ nghi·ªám tr√™n macOS.

## ƒêi·ªÅu ki·ªán ti√™n quy·∫øt:
1. Python 3.8 tr·ªü l√™n (ƒë√£ th·ª≠ nghi·ªám v·ªõi Python 3.11.5)
2. Node.js 16+ v√† npm (ƒë√£ th·ª≠ nghi·ªám v·ªõi Node.js 23.10.0, npm 10.9.2)
3. Docker (t√πy ch·ªçn, ƒë·ªÉ tri·ªÉn khai trong container)
4. RAM 8GB+ (khuy·∫øn ngh·ªã 16GB+)
5. Ollama (b·∫Øt bu·ªôc cho c·∫£ hai c√°ch tri·ªÉn khai)

## GHI CH√ö:
- Tr∆∞·ªõc khi nh√°nh n√†y ƒë∆∞·ª£c di chuy·ªÉn ƒë·∫øn nh√°nh ch√≠nh, vui l√≤ng sao ch√©p nh√°nh n√†y ƒë·ªÉ c√†i ƒë·∫∑t:
```
git clone -b APL-v2 https://github.com/PromtEngineer/localGPT.git
cd localGPT
```
## T√πy ch·ªçn 1: Tri·ªÉn khai Docker
```
# Clone the repository
git clone https://github.com/PromtEngineer/localGPT.git
cd localGPT

# Install Ollama locally (required even for Docker)
curl -fsSL https://ollama.ai/install.sh | sh
ollama pull qwen3:0.6b
ollama pull qwen3:8b

# Start Ollama
ollama serve

# Start with Docker (in a new terminal)
./start-docker.sh

# Access the application
open http://localhost:3000
```
## L·ªánh qu·∫£n l√Ω Docker:
```
# Check container status
docker compose ps

# View logs
docker compose logs -f

# Stop containers
./start-docker.sh stop
```

## L·ª±a ch·ªçn 2: Ph√°t tri·ªÉn tr·ª±c ti·∫øp (Khuy·∫øn ngh·ªã ph√°t tri·ªÉn)
```
# Clone the repository
git clone https://github.com/PromtEngineer/localGPT.git
cd localGPT

# Install Python dependencies
pip install -r requirements.txt

# Key dependencies installed:
# - torch==2.4.1, transformers==4.51.0 (AI models)
# - lancedb (vector database)
# - rank_bm25, fuzzywuzzy (search algorithms)
# - sentence_transformers, rerankers (embedding/reranking)
# - docling (document processing)
# - colpali-engine (multimodal processing - support coming soon)

# Install Node.js dependencies
npm install

# Install and start Ollama
curl -fsSL https://ollama.ai/install.sh | sh
ollama pull qwen3:0.6b
ollama pull qwen3:8b
ollama serve

# Start the system (in a new terminal)
python run_system.py

# Access the application
open http://localhost:3000
```
## Qu·∫£n l√Ω h·ªá th·ªëng:
```
# Check system health (comprehensive diagnostics)
python system_health_check.py

# Check service status and health
python run_system.py --health

# Start in production mode
python run_system.py --mode prod

# Skip frontend (backend + RAG API only)
python run_system.py --no-frontend

# View aggregated logs
python run_system.py --logs-only

# Stop all services
python run_system.py --stop
# Or press Ctrl+C in the terminal running python run_system.py
```
## Ki·∫øn tr√∫c d·ªãch v·ª•: Tr√¨nh run_system.pykh·ªüi ch·∫°y qu·∫£n l√Ω b·ªën d·ªãch v·ª• ch√≠nh:
1. M√°y ch·ªß Ollama (c·ªïng 11434): Ph·ª•c v·ª• m√¥ h√¨nh AI
2. M√°y ch·ªß API RAG (c·ªïng 8001): X·ª≠ l√Ω v√† truy xu·∫•t t√†i li·ªáu
3. M√°y ch·ªß ph·ª• tr·ª£ (c·ªïng 8000): Qu·∫£n l√Ω phi√™n v√† ƒëi·ªÉm cu·ªëi API
4. M√°y ch·ªß Frontend (c·ªïng 3000): Giao di·ªán web React/Next.js

## T√πy ch·ªçn 3: Kh·ªüi ƒë·ªông th√†nh ph·∫ßn th·ªß c√¥ng
```
# Terminal 1: Start Ollama
ollama serve

# Terminal 2: Start RAG API
python -m rag_system.api_server

# Terminal 3: Start Backend
cd backend && python server.py

# Terminal 4: Start Frontend
npm run dev

# Access at http://localhost:3000
```

## C√†i ƒë·∫∑t chi ti·∫øt
1. C√†i ƒë·∫∑t c√°c ph·ª• thu·ªôc h·ªá th·ªëng
Ubuntu/Debian:
```
sudo apt update
sudo apt install python3.8 python3-pip nodejs npm docker.io docker-compose
```
macOS:
```
brew install python@3.8 node npm docker docker-compose
```
C·ª≠a s·ªï:
```
# Install Python 3.8+, Node.js, and Docker Desktop
# Then use PowerShell or WSL2
```
2. C√†i ƒë·∫∑t m√¥ h√¨nh AI
C√†i ƒë·∫∑t Ollama (Khuy·∫øn ngh·ªã):
```
# Install Ollama
curl -fsSL https://ollama.ai/install.sh | sh

# Pull recommended models
ollama pull qwen3:0.6b          # Fast generation model
ollama pull qwen3:8b            # High-quality generation model
```
3. C·∫•u h√¨nh m√¥i tr∆∞·ªùng
```
# Copy environment template
cp .env.example .env

# Edit configuration
nano .env
```
T√πy ch·ªçn c·∫•u h√¨nh ch√≠nh:
```
# AI Models (referenced in rag_system/main.py)
OLLAMA_HOST=http://localhost:11434

# Database Paths (used by backend and RAG system)
DATABASE_PATH=./backend/chat_data.db
VECTOR_DB_PATH=./lancedb

# Server Settings (used by run_system.py)
BACKEND_PORT=8000
FRONTEND_PORT=3000
RAG_API_PORT=8001

# Optional: Override default models
GENERATION_MODEL=qwen3:8b
ENRICHMENT_MODEL=qwen3:0.6b
EMBEDDING_MODEL=Qwen/Qwen3-Embedding-0.6B
RERANKER_MODEL=answerdotai/answerai-colbert-small-v1
```
4. Kh·ªüi t·∫°o h·ªá th·ªëng
```
# Run system health check
python system_health_check.py

# Initialize databases
python -c "from backend.database import ChatDatabase; ChatDatabase().init_database()"

# Test installation
python -c "from rag_system.main import get_agent; print('‚úÖ Installation successful!')"

# Validate complete setup
python run_system.py --health
```

---

## üéØ B·∫Øt ƒë·∫ßu
### 1. T·∫°o ch·ªâ m·ª•c ƒë·∫ßu ti√™n c·ªßa b·∫°n
Ch·ªâ m·ª•c l√† t·∫≠p h·ª£p c√°c t√†i li·ªáu ƒë√£ x·ª≠ l√Ω m√† b·∫°n c√≥ th·ªÉ tr√≤ chuy·ªán.

S·ª≠ d·ª•ng Giao di·ªán Web:
1. M·ªü http://localhost:3000
2. Nh·∫•p v√†o "T·∫°o ch·ªâ m·ª•c m·ªõi"
3. T·∫£i l√™n t√†i li·ªáu c·ªßa b·∫°n (PDF, DOCX, TXT)
4. C·∫•u h√¨nh t√πy ch·ªçn x·ª≠ l√Ω
5. Nh·∫•p v√†o "X√¢y d·ª±ng ch·ªâ m·ª•c"
S·ª≠ d·ª•ng t·∫≠p l·ªánh:
```
# Simple script approach
./simple_create_index.sh "My Documents" "path/to/document.pdf"

# Interactive script
python create_index_script.py
```
S·ª≠ d·ª•ng API:
```
# Create index
curl -X POST http://localhost:8000/indexes \
  -H "Content-Type: application/json" \
  -d '{"name": "My Index", "description": "My documents"}'

# Upload documents
curl -X POST http://localhost:8000/indexes/INDEX_ID/upload \
  -F "files=@document.pdf"

# Build index
curl -X POST http://localhost:8000/indexes/INDEX_ID/build
```
### 2. B·∫Øt ƒë·∫ßu tr√≤ chuy·ªán
Sau khi ch·ªâ m·ª•c c·ªßa b·∫°n ƒë∆∞·ª£c x√¢y d·ª±ng:
1. T·∫°o phi√™n tr√≤ chuy·ªán : Nh·∫•p v√†o "Tr√≤ chuy·ªán m·ªõi" ho·∫∑c s·ª≠ d·ª•ng phi√™n tr√≤ chuy·ªán hi·ªán c√≥
2. Ch·ªçn ch·ªâ m·ª•c c·ªßa b·∫°n : Ch·ªçn b·ªô s∆∞u t·∫≠p t√†i li·ªáu ƒë·ªÉ truy v·∫•n
3. ƒê·∫∑t c√¢u h·ªèi : Nh·∫≠p c√¢u h·ªèi b·∫±ng ng√¥n ng·ªØ t·ª± nhi√™n v·ªÅ t√†i li·ªáu c·ªßa b·∫°n
4. Nh·∫≠n c√¢u tr·∫£ l·ªùi : Nh·∫≠n ph·∫£n h·ªìi do AI t·∫°o ra k√®m theo tr√≠ch d·∫´n ngu·ªìn

### 3. T√≠nh nƒÉng n√¢ng cao
C·∫•u h√¨nh m√¥ h√¨nh t√πy ch·ªânh:
```
# Use different models for different tasks
curl -X POST http://localhost:8000/sessions \
  -H "Content-Type: application/json" \
  -d '{
    "title": "High Quality Session",
    "model": "qwen3:8b",
    "embedding_model": "Qwen/Qwen3-Embedding-4B"
  }'
```
X·ª≠ l√Ω t√†i li·ªáu h√†ng lo·∫°t
```
# Process multiple documents at once
python demo_batch_indexing.py --config batch_indexing_config.json
```

T√≠ch h·ª£p API:
```
import requests

# Chat with your documents via API
response = requests.post('http://localhost:8000/chat', json={
    'query': 'What are the key findings in the research papers?',
    'session_id': 'your-session-id',
    'search_type': 'hybrid',
    'retrieval_k': 20
})

print(response.json()['response'])
```
## üîß C·∫•u h√¨nh
### C·∫•u h√¨nh m√¥ h√¨nh

APL h·ªó tr·ª£ nhi·ªÅu nh√† cung c·∫•p m√¥ h√¨nh AI v·ªõi c·∫•u h√¨nh t·∫≠p trung:
#### 1. M√¥ h√¨nh Ollama (Suy lu·∫≠n c·ª•c b·ªô)
```
OLLAMA_CONFIG = {
    "host": "http://localhost:11434",
    "generation_model": "qwen3:8b",        # Main text generation
    "enrichment_model": "qwen3:0.6b"       # Lightweight routing/enrichment
}
```
#### M√¥ h√¨nh m·∫´u b√™n ngo√†i (HuggingFace Direct)
```
EXTERNAL_MODELS = {
    "embedding_model": "Qwen/Qwen3-Embedding-0.6B",           # 1024 dimensions
    "reranker_model": "answerdotai/answerai-colbert-small-v1", # ColBERT reranker
    "fallback_reranker": "BAAI/bge-reranker-base"             # Backup reranker
}
```
#### C·∫•u h√¨nh ƒë∆∞·ªùng ·ªëng:
APL cung c·∫•p hai c·∫•u h√¨nh ƒë∆∞·ªùng ·ªëng ch√≠nh:

ƒê∆∞·ªùng ·ªëng m·∫∑c ƒë·ªãnh (S·∫µn s√†ng s·∫£n xu·∫•t)
```
"default": {
    "description": "Production-ready pipeline with hybrid search, AI reranking, and verification",
    "storage": {
        "lancedb_uri": "./lancedb",
        "text_table_name": "text_pages_v3",
        "bm25_path": "./index_store/bm25"
    },
    "retrieval": {
        "retriever": "multivector",
        "search_type": "hybrid",
        "late_chunking": {"enabled": True},
        "dense": {"enabled": True, "weight": 0.7},
        "bm25": {"enabled": True}
    },
    "reranker": {
        "enabled": True,
        "type": "ai",
        "strategy": "rerankers-lib",
        "model_name": "answerdotai/answerai-colbert-small-v1",
        "top_k": 10
    },
    "query_decomposition": {"enabled": True, "max_sub_queries": 3},
    "verification": {"enabled": True},
    "retrieval_k": 20,
    "contextual_enricher": {"enabled": True, "window_size": 1}
}
```
#### ƒê∆∞·ªùng ·ªëng nhanh (T·ªëi ∆∞u h√≥a t·ªëc ƒë·ªô)
```
"fast": {
    "description": "Speed-optimized pipeline with minimal overhead",
    "retrieval": {
        "search_type": "vector_only",
        "late_chunking": {"enabled": False}
    },
    "reranker": {"enabled": False},
    "query_decomposition": {"enabled": False},
    "verification": {"enabled": False},
    "retrieval_k": 10,
    "contextual_enricher": {"enabled": False}
}
```
#### C·∫•u h√¨nh t√¨m ki·∫øm:
```
SEARCH_CONFIG = {
    'hybrid': {
        'dense_weight': 0.7,
        'sparse_weight': 0.3,
        'retrieval_k': 20,
        'reranker_top_k': 10
    }
}
```
## üõ†Ô∏è Kh·∫Øc ph·ª•c s·ª± c·ªë:
### C√°c v·∫•n ƒë·ªÅ chung:
#### S·ª± c·ªë c√†i ƒë·∫∑t:
```
# Check Python version
python --version  # Should be 3.8+

# Check dependencies
pip list | grep -E "(torch|transformers|lancedb)"

# Reinstall dependencies
pip install -r requirements.txt --force-reinstall
```
#### C√°c v·∫•n ƒë·ªÅ t·∫£i m√¥ h√¨nh
```
# Check Ollama status
ollama list
curl http://localhost:11434/api/tags

# Pull missing models
ollama pull qwen3:0.6b
```
#### C√°c v·∫•n ƒë·ªÅ v·ªÅ c∆° s·ªü d·ªØ li·ªáu
```
# Check database connectivity
python -c "from backend.database import ChatDatabase; db = ChatDatabase(); print('‚úÖ Database OK')"

# Reset database (WARNING: This deletes all data)
rm backend/chat_data.db
python -c "from backend.database import ChatDatabase; ChatDatabase().init_database()"
```
#### C√°c v·∫•n ƒë·ªÅ v·ªÅ hi·ªáu su·∫•t
```
# Check system resources
python system_health_check.py

# Monitor memory usage
htop  # or Task Manager on Windows

# Optimize for low-memory systems
export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512
```

### Nh·∫≠n tr·ª£ gi√∫p
**1. Ki·ªÉm tra Nh·∫≠t k√Ω** : H·ªá th·ªëng t·∫°o nh·∫≠t k√Ω c√≥ c·∫•u tr√∫c trong logs/th∆∞ m·ª•c:

- logs/system.log: C√°c s·ª± ki·ªán v√† l·ªói h·ªá th·ªëng ch√≠nh
- logs/ollama.log: Nh·∫≠t k√Ω m√°y ch·ªß Ollama
- logs/rag-api.log: Nh·∫≠t k√Ω x·ª≠ l√Ω API RAG
- logs/backend.log: Nh·∫≠t k√Ω m√°y ch·ªß ph·ª• tr·ª£
- logs/frontend.log: Nh·∫≠t k√Ω x√¢y d·ª±ng v√† th·ªùi gian ch·∫°y giao di·ªán ng∆∞·ªùi d√πng

**2. S·ª©c kh·ªèe h·ªá th·ªëng**: Ch·∫°y ch·∫©n ƒëo√°n to√†n di·ªán:
```
python system_health_check.py  # Full system diagnostics
python run_system.py --health  # Service status check
```
**3. ƒêi·ªÉm cu·ªëi v·ªÅ t√¨nh tr·∫°ng s·ª©c kh·ªèe :** Ki·ªÉm tra t√¨nh tr·∫°ng s·ª©c kh·ªèe c·ªßa t·ª´ng d·ªãch v·ª•:

- Ph·∫ßn cu·ªëi:http://localhost:8000/health
- API RAG:http://localhost:8001/health
- Ollama:http://localhost:11434/api/tags

**4. T√†i li·ªáu : Ki·ªÉm tra T√†i li·ªáu K·ªπ thu·∫≠t**: https://github.com/PromtEngineer/localGPT/blob/main/TECHNICAL_DOCS.md 

**5. S·ª± c·ªë GitHub :** B√°o c√°o l·ªói v√† y√™u c·∫ßu t√≠nh nƒÉng

**6. C·ªông ƒë·ªìng :** Tham gia c·ªông ƒë·ªìng Discord/Slack c·ªßa ch√∫ng t√¥i

---

# üîó T√†i li·ªáu tham kh·∫£o API
## ƒêi·ªÉm cu·ªëi c·ªët l√µi
### API tr√≤ chuy·ªán
```
# Session-based chat (recommended)
POST /sessions/{session_id}/chat
Content-Type: application/json

{
  "query": "What are the main topics discussed?",
  "search_type": "hybrid",
  "retrieval_k": 20,
  "ai_rerank": true,
  "context_window_size": 5
}

# Legacy chat endpoint
POST /chat
Content-Type: application/json

{
  "query": "What are the main topics discussed?",
  "session_id": "uuid",
  "search_type": "hybrid",
  "retrieval_k": 20
}
```
### Qu·∫£n l√Ω ch·ªâ m·ª•c:
```
# Create index
POST /indexes
Content-Type: application/json
{
  "name": "My Index",
  "description": "Description",
  "config": "default"
}

# Get all indexes
GET /indexes

# Get specific index
GET /indexes/{id}

# Upload documents to index
POST /indexes/{id}/upload
Content-Type: multipart/form-data
files: [file1.pdf, file2.pdf, ...]

# Build index (process uploaded documents)
POST /indexes/{id}/build
Content-Type: application/json
{
  "config_mode": "default",
  "enable_enrich": true,
  "chunk_size": 512
}

# Delete index
DELETE /indexes/{id}
```
### Qu·∫£n l√Ω phi√™n:
```
# Create session
POST /sessions
Content-Type: application/json
{
  "title": "My Session",
  "model": "qwen3:0.6b"
}

# Get all sessions
GET /sessions

# Get specific session
GET /sessions/{session_id}

# Get session documents
GET /sessions/{session_id}/documents

# Get session indexes
GET /sessions/{session_id}/indexes

# Link index to session
POST /sessions/{session_id}/indexes/{index_id}

# Delete session
DELETE /sessions/{session_id}

# Rename session
POST /sessions/{session_id}/rename
Content-Type: application/json
{
  "new_title": "Updated Session Name"
}
```

### T√≠nh nƒÉng n√¢ng cao
#### Ph√¢n t√≠ch truy v·∫•n
H·ªá th·ªëng c√≥ th·ªÉ chia c√°c truy v·∫•n ph·ª©c t·∫°p th√†nh c√°c c√¢u h·ªèi nh·ªè h∆°n ƒë·ªÉ c√≥ c√¢u tr·∫£ l·ªùi t·ªët h∆°n:
```
POST /sessions/{session_id}/chat
Content-Type: application/json

{
  "query": "Compare the methodologies and analyze their effectiveness",
  "query_decompose": true,
  "compose_sub_answers": true
}
```
#### X√°c minh c√¢u tr·∫£ l·ªùi
X√°c minh ƒë·ªôc l·∫≠p v·ªÅ ƒë·ªô ch√≠nh x√°c b·∫±ng c√°ch s·ª≠ d·ª•ng m√¥ h√¨nh x√°c minh ri√™ng bi·ªát:
```
POST /sessions/{session_id}/chat
Content-Type: application/json

{
  "query": "What are the key findings?",
  "verify": true
}
```
#### L√†m gi√†u ng·ªØ c·∫£nh
L√†m gi√†u ng·ªØ c·∫£nh t√†i li·ªáu trong qu√° tr√¨nh l·∫≠p ch·ªâ m·ª•c ƒë·ªÉ hi·ªÉu r√µ h∆°n:
```
# Enable during index building
POST /indexes/{id}/build
{
  "enable_enrich": true,
  "window_size": 2
}
```
#### Ph√¢n ƒëo·∫°n mu·ªôn
B·∫£o to√†n ng·ªØ c·∫£nh t·ªët h∆°n b·∫±ng c√°ch ph√¢n ƒëo·∫°n sau khi nh√∫ng:
```
# Configure in pipeline
"late_chunking": {"enabled": true}
```
#### Tr√≤ chuy·ªán tr·ª±c tuy·∫øn
```
POST /chat/stream
Content-Type: application/json

{
  "query": "Explain the methodology",
  "session_id": "uuid",
  "stream": true
}
```
#### X·ª≠ l√Ω h√†ng lo·∫°t
```
# Using the batch indexing script
python demo_batch_indexing.py --config batch_indexing_config.json

# Example batch configuration (batch_indexing_config.json):
{
  "index_name": "Sample Batch Index",
  "index_description": "Example batch index configuration",
  "documents": [
    "./rag_system/documents/invoice_1039.pdf",
    "./rag_system/documents/invoice_1041.pdf"
  ],
  "processing": {
    "chunk_size": 512,
    "chunk_overlap": 64,
    "enable_enrich": true,
    "enable_latechunk": true,
    "enable_docling": true,
    "embedding_model": "Qwen/Qwen3-Embedding-0.6B",
    "generation_model": "qwen3:0.6b",
    "retrieval_mode": "hybrid",
    "window_size": 2
  }
}
```
```
# API endpoint for batch processing
POST /batch/index
Content-Type: application/json

{
  "file_paths": ["doc1.pdf", "doc2.pdf"],
  "config": {
    "chunk_size": 512,
    "enable_enrich": true,
    "enable_latechunk": true,
    "enable_docling": true
  }
}
```
ƒê·ªÉ bi·∫øt t√†i li·ªáu API ƒë·∫ßy ƒë·ªß, h√£y xem API_REFERENCE.md : https://github.com/PromtEngineer/localGPT/blob/main/API_REFERENCE.md.

# üèóÔ∏è Ki·∫øn tr√∫c
APL ƒë∆∞·ª£c x√¢y d·ª±ng theo ki·∫øn tr√∫c m√¥-ƒëun, c√≥ kh·∫£ nƒÉng m·ªü r·ªông:

```mermaid
graph TB
    UI[Giao di·ªán web] --> API[API Database]
    API --> Agent[ƒê·∫°i l√Ω RAG]
    Agent --> Retrieval[Retrieval Pipeline]
    Agent --> Generation[Generation Pipeline]

    Retrieval --> Vector[T√¨m ki·∫øm Vector]
    Retrieval --> BM25[T√¨m ki·∫øm BM25]
    Retrieval --> Rerank[X·∫øp h·∫°ng l·∫°i]

    Vector --> LanceDB[(LanceDB)]
    BM25 --> BM25DB[(M·ª•c l·ª•c BM25)]

    Generation --> Ollama[M√¥ h√¨nh Ollama]
    Generation --> HF[M√¥ h√¨nh tr√™n HGF]

    API --> SQLite[(C∆° s·ªü d·ªØ li·ªáu SQLite)]
```
T·ªïng quan v·ªÅ t√°c nh√¢n truy xu·∫•t

```mermaid
graph TB
    classDef llmcall fill:#e6f3ff,stroke:#007bff;
    classDef pipeline fill:#e6ffe6,stroke:#28a745;
    classDef cache fill:#fff3e0,stroke:#fd7e14;
    classDef logic fill:#f8f9fa,stroke:#6c757d;
    classDef thread stroke-dasharray: 5 5;

    A(B·∫Øt ƒë·∫ßu: Agent.run) --> B_asyncio.run(_run_async);
    B --> C{_run_async};

    C --> C1[L·∫•y l·ªãch s·ª≠ tr√≤ chuy·ªán];
    C1 --> T1[X√¢y d·ª±ng l·ªùi nh·∫Øc ph√¢n lo·∫°i <br/> Truy v·∫•n + T·ªïng quan t√†i li·ªáu];
    T1 --> T2["(asyncio.to_thread)<br/>Ph√¢n lo·∫°i LLM: RAG hay LLM_DIRECT?"]; class T2 llmcall,thread;
    T2 --> T3{Quy·∫øt ƒë·ªãnh?};

    T3 -- RAG --> RAG_Path;
    T3 -- LLM_DIRECT --> LLM_Path;

    subgraph RAG Path
        RAG_Path --> R1["ƒê·ªãnh d·∫°ng truy v·∫•n + L·ªãch s·ª≠"];
        R1 --> R2["(asyncio.to_thread)<br/>T·∫°o nh√∫ng truy v·∫•n"]; class R2 pipeline,thread;
        R2 --> R3{{Ki·ªÉm tra b·ªô ƒë·ªám ng·ªØ nghƒ©a}};  class R3 cache;
        R3 -- Hit --> R_Cache_Hit("Tr·∫£ v·ªÅ k·∫øt qu·∫£ trong b·ªô nh·ªõ ƒë·ªám");
        R_Cache_Hit --> R_Hist_Update;
        R3 -- Miss --> R4{Ph√¢n h·ªßy <br/> ƒê√£ b·∫≠t?};

        R4 -- Yes --> R5["(asyncio.to_thread)<br/>Ph√¢n t√≠ch truy v·∫•n th√¥"]; class R5 llmcall,thread;
        R5 --> R6{{Ch·∫°y truy v·∫•n ph·ª• <br/> ƒê∆∞·ªùng ·ªëng RAG song song}}; class R6 pipeline,thread;
        R6 --> R7[Thu th·∫≠p k·∫øt qu·∫£ v√† t√†i li·ªáu];
        R7 --> R8["(asyncio.to_thread)<br/>So·∫°n c√¢u tr·∫£ l·ªùi cu·ªëi c√πng"]; class R8 llmcall,thread;
        R8 --> V1(C√¢u tr·∫£ l·ªùi c·ªßa RAG);

        R4 -- No --> R9["(asyncio.to_thread)<br/>Ch·∫°y truy v·∫•n ƒë∆°n <br/>(RAG Pipeline)"]; class R9 pipeline,thread;
        R9 --> V1;

        V1 --> V2{{X√°c minh <br/> await verify_async}}; class V2 llmcall;
        V2 --> V3("K·∫øt qu·∫£ RAG cu·ªëi c√πng");
        V3 --> R_Cache_Store{{L∆∞u tr·ªØ trong b·ªô nh·ªõ ƒë·ªám ng·ªØ nghƒ©a}}; class R_Cache_Store cache;
        R_Cache_Store --> FinalResult;
    end

    subgraph Direct LLM Path
        LLM_Path --> L1["ƒê·ªãnh d·∫°ng truy v·∫•n + L·ªãch s·ª≠"];
        L1 --> L2["(asyncio.to_thread)<br/>T·∫°o c√¢u tr·∫£ l·ªùi LLM tr·ª±c ti·∫øp <br/> (Kh√¥ng c√≥ RAG)"]; class L2 llmcall,thread;
        L2 --> FinalResult("K·∫øt qu·∫£ tr·ª±c ti·∫øp cu·ªëi c√πng");
    end

    FinalResult --> R_Hist_Update("C·∫≠p nh·∫≠t l·ªãch s·ª≠ tr√≤ chuy·ªán");
    R_Hist_Update --> ZZZ("K·∫øt th√∫c: Tr·∫£ v·ªÅ k·∫øt qu·∫£");
```

# ü§ù ƒê√≥ng g√≥p:
Ch√∫ng t√¥i hoan ngh√™nh s·ª± ƒë√≥ng g√≥p t·ª´ c√°c nh√† ph√°t tri·ªÉn ·ªü m·ªçi tr√¨nh ƒë·ªô! APL l√† m·ªôt d·ª± √°n ngu·ªìn m·ªü ƒë∆∞·ª£c h∆∞·ªüng l·ª£i t·ª´ s·ª± tham gia c·ªßa c·ªông ƒë·ªìng.

## üöÄ B·∫Øt ƒë·∫ßu nhanh cho Ng∆∞·ªùi ƒë√≥ng g√≥p
```
# Fork and clone the repository
git clone https://github.com/PromtEngineer/APL.git
cd APL

# Set up development environment
pip install -r requirements.txt
npm install

# Install Ollama and models
curl -fsSL https://ollama.ai/install.sh | sh
ollama pull qwen3:0.6b qwen3:8b

# Verify setup
python system_health_check.py
python run_system.py --mode dev
```
### üìã C√°ch ƒë√≥ng g√≥p
1. üêõ B√°o c√°o l·ªói : S·ª≠d·ª•ng m·∫´u b√°o c√°o l·ªói c·ªßa ch√∫ng t√¥i https://github.com/PromtEngineer/localGPT/blob/main/.github/ISSUE_TEMPLATE/bug_report.md
2. üí° Y√™u c·∫ßu t√≠nh nƒÉng : S·ª≠ d·ª•ng m·∫´u y√™u c·∫ßu t√≠nh nƒÉng c·ªßa ch√∫ng t√¥i https://github.com/PromtEngineer/localGPT/blob/main/.github/ISSUE_TEMPLATE/feature_request.md
3. üîß G·ª≠i m√£ : Theo d√µi quy tr√¨nh ph√°t tri·ªÉn c·ªßa ch√∫ng t√¥i: https://github.com/PromtEngineer/localGPT/blob/main/CONTRIBUTING.md#development-workflow
4. üìö C·∫£i thi·ªán T√†i li·ªáu : Gi√∫p c·∫£i thi·ªán t√†i li·ªáu c·ªßa ch√∫ng t√¥i

### üìñ H∆∞·ªõng d·∫´n chi ti·∫øt
ƒê·ªÉ bi·∫øt h∆∞·ªõng d·∫´n ƒë√≥ng g√≥p to√†n di·ªán, bao g·ªìm:

1. Thi·∫øt l·∫≠p ph√°t tri·ªÉn v√† quy tr√¨nh l√†m vi·ªác
2. Ti√™u chu·∫©n m√£ h√≥a v√† th·ª±c h√†nh t·ªët nh·∫•t
3. Y√™u c·∫ßu th·ª≠ nghi·ªám
4. Ti√™u chu·∫©n t√†i li·ªáu
5. Qu√° tr√¨nh ph√°t h√†nh
üëâ Xem h∆∞·ªõng d·∫´n CONTRIBUTING.md c·ªßa ch√∫ng t√¥i: https://github.com/PromtEngineer/localGPT/blob/main/CONTRIBUTING.md

### üìÑ Gi·∫•y ph√©p
D·ª± √°n n√†y ƒë∆∞·ª£c c·∫•p ph√©p theo Gi·∫•y ph√©p MIT - xem t·ªáp LICENSE https://github.com/PromtEngineer/localGPT/blob/main/LICENSE ƒë·ªÉ bi·∫øt chi ti·∫øt. 
ƒê·ªëi v·ªõi c√°c m√¥ h√¨nh, vui l√≤ng ki·ªÉm tra gi·∫•y ph√©p t∆∞∆°ng ·ª©ng.

### üìû H·ªó tr·ª£
1. T√†i li·ªáu : T√†i li·ªáu k·ªπ thu·∫≠t: https://github.com/PromtEngineer/localGPT/blob/main/TECHNICAL_DOCS.md
2. C√°c v·∫•n ƒë·ªÅ : C√°c v·∫•n ƒë·ªÅ c·ªßa GitHub: https://github.com/PromtEngineer/localGPT/issues
3. Th·∫£o lu·∫≠n : Th·∫£o lu·∫≠n GitHub: https://github.com/PromtEngineer/localGPT/discussions
4. Tri·ªÉn khai v√† t√πy ch·ªânh doanh nghi·ªáp : Li√™n h·ªá v·ªõi ch√∫ng t√¥i: https://tally.so/r/wv6R2d
#### L·ªãch s·ª≠ ng√¥i sao: star-history.com 
